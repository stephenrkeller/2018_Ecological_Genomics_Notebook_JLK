# Jamie -- Impressive notebook. Extremely well organized and annotated. --Steve


## Author: Jamie Kostyun
## Ecological Genomics:   

### Overall Description of notebook      

This notebook will document my entries throughout the semester.


### Date started: 2018-01-24  
### Date end:   (year-month-day)    

### Philosophy   
Science should be reproducible and one of the best ways to achieve this is by logging research activities in a notebook. Because science/biology has increasingly become computational, it is easier to document computational projects in an electronic form, which can be shared online through Github.    

### Helpful features of the notebook     

**It is absolutely critical for your future self and others to follow your work.**     

* The notebook is set up with a series of internal links from the table of contents.    
* All notebooks should have a table of contents which has the "Page", date, and title (information that allows the reader to understand your work).     
* Also, one of the perks of keeping all activities in a single document is that you can **search and find elements quickly**.     
* Lastly, you can share specific entries because of the three "#" automatically creates a link when the notebook renders on github.      


<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.  


### Table of contents for 60 entries (Format is *Page: Date(with year-month-day). Title*)        
* [Page 1: 2018-01-24](#id-section1). Intro to Github, RMarkdown, and UNIX command-line
* [Page 2: 2018-01-26](#id-section2). Practice with UNIX commands
* [Page 3: 2018-01-29](#id-section3). Introduction to handling RNAseq data
* [Page 4: 2018-01-31](#id-section4). Continuing with RNAseq data: filtering and mapping to reference
* [Page 5: 2018-02-05 and 2018-02-06](#id-section5). Examining SAM file and troubleshooting 
* [Page 6: 2018-02-07](#id-section6). Exploring expression differences using DESeq2
* [Page 7: 2018-02-14](#id-section7). Using DESeq2 to assess differential gene expression
* [Page 8: 2018-02-21](#id-section8). Continuing with DESEq2 and also clustering/module ID with WGCNA
* [Page 9: 2018-03-05](#id-section9). Code for Assignment 1 analysis
* [Page 10: 2018-02-26 and 2018-02-28](#id-section10). Intro to population genomics
* [Page 11: 2018-03-05](#id-section11). Continuing with working with vcf files
* [Page 12: 2018-03-07](#id-section12). Continuing with exploring allele frequencies
* [Page 13: 2018-03-19](#id-section13). Using SNP data to examine population structure
* [Page 14: 2018-03-21](#id-section14). Using Bayescan to identify loci under differential selection
* [Page 15: 2018-03-22](#id-section15). Code for Assignment 2 analysis
* [Page 16: 2018-03-26](#id-section16). Continuing with Bayescan
* [Page 17: 2018-03-28](#id-section17). Identifying outliers from Bayescan
* [Page 18: 2018-04-13](#id-section18). Code for class project
* [Page 19: 2018-04-16](#id-section19). Continuing analysis for class project
* [Page 20: 2018-04-17](#id-section20). Code for class project, continued
* [Page 21: 2018-04-18](#id-section21). Code for class project, continued
* [Page 22: 2018-04-19](#id-section22). Class project (Calling ancestral vs. derived alleles)
* [Page 23:](#id-section23).
* [Page 24:](#id-section24).
* [Page 25:](#id-section25).
* [Page 26:](#id-section26).
* [Page 27:](#id-section27).
* [Page 28:](#id-section28).
* [Page 29:](#id-section29).
* [Page 30:](#id-section30).
* [Page 31:](#id-section31).
* [Page 32:](#id-section32).
* [Page 33:](#id-section33).
* [Page 34:](#id-section34).
* [Page 35:](#id-section35).
* [Page 36:](#id-section36).
* [Page 37:](#id-section37).
* [Page 38:](#id-section38).
* [Page 39:](#id-section39).
* [Page 40:](#id-section40).
* [Page 41:](#id-section41).
* [Page 42:](#id-section42).
* [Page 43:](#id-section43).
* [Page 44:](#id-section44).
* [Page 45:](#id-section45).
* [Page 46:](#id-section46).
* [Page 47:](#id-section47).
* [Page 48:](#id-section48).
* [Page 49:](#id-section49).
* [Page 50:](#id-section50).
* [Page 51:](#id-section51).
* [Page 52:](#id-section52).
* [Page 53:](#id-section53).
* [Page 54:](#id-section54).
* [Page 55:](#id-section55).
* [Page 56:](#id-section56).
* [Page 57:](#id-section57).
* [Page 58:](#id-section58).
* [Page 59:](#id-section59).
* [Page 60:](#id-section60).

------
<div id='id-section1'/>
### Page 1: 2018-01-24. Notes on using Github, Rmarkdown, and the UNIX command-line.

Today, we created Github repo for the course, and started the course Notebook. 

Other goals:

* publish notebook to Github
* log into UNIX server

------
<div id='id-section2'/>
### Page 2: 2018-01-26. Practice with UNIX commands. 

Today, I practiced using UNIX commands in the Terminal, including modifying files in my home directory and connecting back to the class server. I also followed the course tutorial section about editing my settings, so as to be asked if I actually want to delete files. I think this change will be very helpful!

------
<div id='id-section3'/>
### Page 3: 2018-01-29. Introduction to handling RNAseq data

Today, we started handling RNAseq data: began learning the RNA processing pipeline, how to write a bash script, and how to interpret fastq files. 

We practiced with RNAseq data from horned beetles, where different populations have diverged in their relationship b/t male body size and horn size - i.e. different thresholds. 

First, I connected to the class server and navigated to the data directory:

      cd /data/project_data/beetles/rawdata/

...from which I examined the beginning of a fastq file:

      zcat WA_PP1_F2_AGTTCC_L003_R1_001.fastq.gz | head -n4

To examine the overall/summary quality scores for all reads in this sample (Western Australia, Female2, Paired-End1), I first created a new directory in my home directory to hold working RNAseq files. Then, I used commands from FastQ to create the quality report and have it output to my new directory:

      cd ~
      mkdir 2018_clean_reads
      fastqc /data/project_data/beetles/rawdata/WA_PP1_F2_AGTTCC_L003_R1_001.fastq.gz -o .

This output included a .zip file and an .html file. Because we will be using reads from both paired-end sets, I did the same thing with the other file:

      fastqc /data/project_data/beetles/rawdata/WA_PP1_F2_AGTTCC_L003_R2_001.fastq.gz -o .

Using cyberduck, I then moved these files to a data folder on my personal computer. Next, I opend the .html files to examine the quality scores for both sets. Overall, both sets look pretty good although PE2 has more poorer quality base calls near the end of the read. Both also have residue from the random hexamer primers at the front of the read as well as some adaptor sequence near the end - both of these issues will be fixed when we trim the reads. 

------
<div id='id-section4'/> 
### Page 4: 2018-01-31. Continuing with RNAseq data: filtering and mapping to reference

Today, we continued dealing with beetle RNAseq data. After examining the overall quality with the FastQ reports, we used the program Trimmomatic to trim and filter reads. To do so, we first copied an example script for Trimmomatic from the course materials to our home directory (into myscripts directory):

      cp /data/scripts/trim_example.sh ~/myscripts/
      
We then used the program Vim to edit the example script within the command window:

      vim trim_example.sh
      
Inside the vim edit window, this is what the example script looked like:

      #!/bin/bash
        java -classpath /data/popgen/Trimmomatic-0.33/trimmomatic-0.33.jar org.usadellab.trimmomatic.TrimmomaticPE \
                -threads 1 \ 
                -phred33 \
                 /data/project_data/beetles/rawdata/WA_PP1_YOURSAMPLE_R1.fastq.gz \
                 /data/project_data/beetles/rawdata/WA_PP1_YOURSAMPLE_R2.fastq.gz \
                 ~/cleanreads/"YOURSAMPLE_R1_clean_paired.fa" \
                 ~/cleanreads/"YOURSAMPLE_R1_clean_unpaired.fa" \
                 ~/cleanreads/"YOURSAMPLE_R2_clean_paired.fa" \
                 ~/cleanreads/"YOURSAMPLE_R2_clean_unpaired.fa" \
                 ILLUMINACLIP:/data/popgen/Trimmomatic-0.33/adapters/TruSeq3-PE.fa:2:30:10 \
                 LEADING:28 \
                TRAILING:28 \
                SLIDINGWINDOW:6:28 \
                HEADCROP:12 \
                MINLEN:35 \
                
This is specifying that it's a Java based program, where to find it (in the shared class space in data/popgen), and that we'll be using it to analyze paired end data. After the -phred33 \, we specify the input files - there are two because we're doing paired end analysis. Next, it lists the output files for the cleaned/trimmed reads. Finally, it lists the steps of functions to perform on the raw reads. Here's more info on these steps:

ILLUMINACLIP: Cut adapter and other illumina-specific sequences from the read.
SLIDINGWINDOW: Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold.
LEADING: Cut bases off the start of a read, if below a threshold quality
TRAILING: Cut bases off the end of a read, if below a threshold quality
CROP: Cut the read to a specified length
HEADCROP: Cut the specified number of bases from the start of the read
MINLEN: Drop the read if it is below a specified length

To edit the script in vim, need to type "i" for INSERT. After changes are complete, hit escape key to exit edit mode. To enter general commands, type ":" followed by the command. For example, ":w" means write the changes to the file, and ":q" means quit vim.

After editing, this is what my Trimmomatic script looked like:

      #!/bin/bash
        java -classpath /data/popgen/Trimmomatic-0.33/trimmomatic-0.33.jar org.usadellab.trimmomatic.TrimmomaticPE \
                      -threads 1 \
                      -phred33 \
                       /data/project_data/beetles/rawdata/WA_PP1_F2_AGTTCC_L003_R1_001.fastq.gz \
                       /data/project_data/beetles/rawdata/WA_PP1_F2_AGTTCC_L003_R2_001.fastq.gz \
                       ~/2018_clean_reads/"WA_PP1_F2_R1_clean_paired.fa" \
                       ~/2018_clean_reads/"WA_PP1_F2_R1_clean_unpaired.fa" \
                       ~/2018_clean_reads/"WA_PP1_F2_R2_clean_paired.fa" \
                       ~/2018_clean_reads/"WA_PP1_F2_R2_clean_unpaired.fa" \
                       ILLUMINACLIP:/data/popgen/Trimmomatic-0.33/adapters/TruSeq3-PE.fa:2:30:10 \
                       LEADING:28 \
                   TRAILING:28 \
                   SLIDINGWINDOW:6:28 \
                   HEADCROP:12 \
                   MINLEN:35 \

I then changed the filename, and also changed the user permissions so I could execute it. Here, r means readable, w means writeable, and x means executable:

      mv trim_example.sh trim_Fl.sh
      chmod u+x trim_Fl.sh
      
While still in the myscripts directory, I then executed the script:

      ./trim_Fl.sh 
      
Output inside terminal window:

      TrimmomaticPE: Started with arguments: -threads 1 -phred33        /data/project_data/beetles/rawdata/WA_PP1_F2_AGTTCC_L003_R1_001.fastq.gz /data/project_data/beetles/rawdata/WA_PP1_F2_AGTTCC_L003_R2_001.fastq.gz /users/j/k/jkostyun/2018_clean_reads/WA_PP1_F2_R1_clean_paired.fa /users/j/k/jkostyun/2018_clean_reads/WA_PP1_F2_R1_clean_unpaired.fa /users/j/k/jkostyun/2018_clean_reads/WA_PP1_F2_R2_clean_paired.fa /users/j/k/jkostyun/2018_clean_reads/WA_PP1_F2_R2_clean_unpaired.fa ILLUMINACLIP:/data/popgen/Trimmomatic-0.33/adapters/TruSeq3-PE.fa:2:30:10 LEADING:28 TRAILING:28 SLIDINGWINDOW:6:28 HEADCROP:12 MINLEN:35
      Using PrefixPair: 'TACACTCTTTCCCTACACGACGCTCTTCCGATCT' and 'GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT'
      ILLUMINACLIP: Using 1 prefix pairs, 0 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences
      Input Read Pairs: 16000000 Both Surviving: 10714663 (66.97%) Forward Only Surviving: 2565021 (16.03%) Reverse       Only Surviving: 734046 (4.59%) Dropped: 1986270 (12.41%)
      TrimmomaticPE: Completed successfully
      
To make sure that the script actually did what we thought it was going to, we ran FastQ on the clean reads file:

      cd 2018_clean_reads
      fastqc ~/2018_clean_reads/WA_PP1_F2_R1_clean_paired.fa
      fastqc ~/2018_clean_reads/WA_PP1_F2_R2_clean_paired.fa
      
Once satisfied with the improved quality, we then started preparing to map the clean reads against the reference. The reference for O. taurus is already available in the shared course space. To first index the file (only needs to be done once), we used the program bwa:

      bwa index /data/project_data/beetles/reference/OTAU.fna
      
Then we mapped clean reads to the reference, and output (sam file).

      bwa mem /data/project_data/beetles/reference/OTAU.fna  ~/2018_clean_reads/WA_PP1_F2_R1_clean_paired.fa ~/2018_clean_reads/WA_PP1_F2_R2_clean_paired.fa > WA_PP1_F2_bwamem.sam

------
<div id='id-section5'/>
### Page 5: 2018-02-05 and 2018-02-06. Examining SAM file and troubleshooting

Today at the beginning of class, we used the -tail function to start examining our SAM file:
      
      tail -n 100 WA_PP1_F2_bwamem.sam > tail.PP1_F2.sam
      vim tail.PP1_F2.sam
      
However, when I went to examine it, it was an empty file. So was my SAM file. Clearly there was a problem with my mapping command/output, so I followed along in class using a SAM file available in the shared class space.

But now I realized that my actual SAM file was just in another folder, and the empty one was from a first attempt at mapping where I got an error message. So I moved the correct file to the correct folder. Problem solved.

After looking over the vim field, just to get a sense of what's included in a SAM file, we assessed overall mapping quality:

      samtools flagstat WA_PP1_F2_bwamem.sam
      
Output inside terminal window:

      21506273 + 0 in total (QC-passed reads + QC-failed reads)
      0 + 0 secondary
      76947 + 0 supplementary
      0 + 0 duplicates
      17749530 + 0 mapped (82.53% : N/A)
      21429326 + 0 paired in sequencing
      10714663 + 0 read1
      10714663 + 0 read2
      17100460 + 0 properly paired (79.80% : N/A)
      17369268 + 0 with itself and mate mapped
      303315 + 0 singletons (1.42% : N/A)
      183336 + 0 with mate mapped to a different chr
      151063 + 0 with mate mapped to a different chr (mapQ>=5)
      
Next we used a python script to count reads mapping to different contigs/"genes":

      python countxpression_PE.py 20 35 ~/mydata/sam/PP1_F2_countstatssummary.txt ~/mydata/sam/WA_PP1_F2_bwamem.sam
      
However, the file only contains 0 for all reads, suggesting there is something wrong with the script. Indeed, it seems that the mem function in BWA (which we used for mapping) may have somewhat different flags than the align function (which is what the script was written for).
      
------
<div id='id-section6'/>
### Page 6: 2018-02-07. Exploring expression differences using DESeq2.

Today, we continued our exploration and analysis of beetle RNAseq data. Last time, the script we were going to use to get count data didn't work on the BWA mem function. So Melissa provided an alternate file for us that used the BWA align function to get counts, and then a bash script to combine read counts from all individuals into a single file.

First, we copied the relevant files from the course space. Typically I would just use CyyberDuck, but here I tried this:

    scp jkostyun@pbio381.uvm.edu:/data/project_data/beetles/counts/* .
    
Fo this command to work, I was in the directory on my computer that I wanted to transfer the files to (indicated by the "."). This copied two files "allcountsdataRN_noIT.txt" and "cols_data_noIT.txt".

Next, we loaded the DESeq2 package in R(Studio) and started writing a script to use for differential expression analyses. 

      #set working directory
      library("DESeq2")
      library("ggplot2")

      countsTable = read.delim('allcountsdataRN_noIT.txt', header=T, stringsAsFactors = T, row.names=1)
      countData = as.matrix(countsTable)
      head(countsData)

      conds = read.delim('cols_data_noIT.txt', header=T, stringsAsFactors = T, row.names=1)
      head(conds)
      colData = as.data.frame(conds)

      ################

      dds = DESeqDataSetFromMatrix(countData = countData, colData = colData, design = ~ devstage + sex + population)
      # DEseq2 interprets models as testing differences b/t last predictor accounting for previous predictors

      dim(dds) #gives summary of input: 17483    48

      dds1 = dds[rowSums(counts(dds)) > 1]  #revises dds to exclude transcripts with 0 reads, as previous analysis        included all transcripts
      dim(dds1) #gives updated summary: 16851    48

      dds1 = DESeq(dds1, modelMatrixType = "standard") #this is the primary command to compare transcript counts

      resultsNames(dds1)
      #[1] "Intercept" "devstage_L3L_vs_AD4" "devstage_PD1_vs_AD4" "devstage_PP1_vs_AD4" "sex_M_vs_F"         
      #[6] "population_WA_vs_NC"

      res = results(dds1)  #this aggregates output of DESeq command
      res = res[order(res$padj),] #this re-orders output by adjusted p-value
      head(res)

      #log2 fold change (MAP): population WA vs NC 
      #Wald test p-value: population WA vs NC 
      #DataFrame with 6 rows and 6 columns
      # baseMean log2FoldChange     lfcSE      stat       pvalue         padj
      # <numeric>      <numeric> <numeric> <numeric>    <numeric>    <numeric>
      # OTAU008667-RA 231.87111     -0.7803753 0.1132661 -6.889755 5.588875e-12 4.220159e-08
      # OTAU012562-RA 251.77436     -0.7828048 0.1135355 -6.894800 5.394059e-12 4.220159e-08
      # OTAU011160-RA  10.24152     -1.0863185 0.1836927 -5.913781 3.343429e-09 1.470408e-05
      # OTAU012716-RA 188.87768      1.0137238 0.1721500  5.888609 3.894604e-09 1.470408e-05
      # OTAU002976-RA 998.42315     -0.6159655 0.1075972 -5.724735 1.035950e-08 3.128984e-05
      # OTAU014686-RA 603.66091     -0.8168345 0.1468336 -5.562995 2.651835e-08 6.674668e-05

      summary(res)
      # out of 16851 with nonzero total read count
      # adjusted p-value < 0.1
      # LFC > 0 (up)     : 264, 1.6% 
      # LFC < 0 (down)   : 245, 1.5% 
      # outliers [1]     : 133, 0.79% 
      # low counts [2]   : 1616, 9.6% 
      # (mean count < 1)
      # [1] see 'cooksCutoff' argument of ?results
      # [2] see 'independentFiltering' argument of ?results



------
<div id='id-section7'/>
### Page 7: 2018-02-14. Using DESeq2 to assess differential gene expression. 

Today, we continued using DESeq2 to examine differences in gene expression between beetle populations, sex, and developmental stage. We also started examining different tools for visualizing these data.

Here is the additional code:

      ###Data Visualization###

      plotMA(res, main="DESeq2", ylim=c(-2,2)) #this plots population comparison
      abline(h=c(-1,1), col="blue", lwd = 2)

      # sex effect?

      res_sex <- results(dds1, name="sex_M_vs_F", alpha=0.05) #need to rerun model above looking at sex instead of population
      plotMA(res_sex, main="DESeq2", ylim=c(-15,15))
      abline(h=c(-1,1), col="blue", lwd = 2)

      #PCA to get a global view of gene expression
      rld <- rlog(dds1, blind=FALSE) #takes a while
      data <- plotPCA(rld, intgroup=c("population","devstage","sex"), returnData=TRUE)
      percentVar <- round(100 * attr(data, "percentVar"))

      data$devstage <- factor(data$devstage, levels=c("L3L","PP1","PD1","AD4"), labels=c("L3L","PP1","PD1","AD4"))

      ggplot(data, aes(PC1, PC2, color=sex, shape=devstage)) +
        geom_point(size=4, alpha=0.85) +
        xlab(paste0("PC1: ",percentVar[1],"% variance")) +
        ylab(paste0("PC2: ",percentVar[2],"% variance")) +
        theme_minimal()

      ggplot(data, aes(PC1, PC2, color=population, shape=devstage)) +
        geom_point(size=4, alpha=0.85) +
        xlab(paste0("PC1: ",percentVar[1],"% variance")) +
        ylab(paste0("PC2: ",percentVar[2],"% variance")) +
        theme_minimal()


      #this section will make a heatmap of all the samples vs all samples

      library("RColorBrewer")
      library("pheatmap")

      sampleDists = dist(t(assay(rld)))
      sampleDistMatrix = as.matrix(sampleDists)
      rownames(sampleDistMatrix) = paste(rld$population, rld$devstage, rld$sex, sep="-")
      colnames(sampleDistMatrix) = NULL
      colors = colorRampPalette(rev(brewer.pal(9, "Blues")))(255)

      pheatmap(sampleDistMatrix, clustering_distance_rows = sampleDists, clustering_distance_cols = sampleDists, col=colors)

      #examining specific transcripts more closely

      d = plotCounts(dds1, gene="OTAU014686-RA", intgroup= (c("population", "sex", "devstage")), returnData = T)
      p = ggplot(d, aes(x=devstage, y=count, shape=sex, color=population)) + theme_minimal() +   theme(text=element_text(size=20), panel.grid.major=element_line(color="grey"))
      p = p + geom_point(position=position_jitter(w=0.3, h=0), size=3) + scale_x_discrete(limits=c("L3L","PP1","PD1","AD4"))


      ###Save outputs###

      write.csv(res, file="DGE_NCvsWA_pop.csv", row.names=T, quote=F) #this outputs results as csv file

      neglogpval = as.matrix(-log(res$pvalue))   #script for GO Mann-Whitney test requires this format
      head(neglogpval)
      res_negpval = cbind(row.names(res), neglogpval)
      colnames(res_negpval)=c("gene", "neglogpval")

      write.csv(res_negpval, file="DGE_NCvsWA_pop_neglogpval.csv.csv", row.names=F, quote=F)


Next, we started the process for GO term assignment and assessment of significant category enrichment. For this, the required files (scripts and databases) were provided to us in the shared course space. To use them, I moved them onto my computer to run in R. I also moved the DESeq2 output file into the same directory on my computer, making sure it was in the right format. 

      input="DGE_NCvsWA_pop_neglogpval.csv" # two columns of comma-separated values: gene id, continuous measure of significance. To perform standard GO enrichment analysis based on Fisher's exact test, use binary measure (0 or 1, i.e., either sgnificant or not).
      ## Make sure this is saved as Unix (LF) - open in TextWrangler, save as, change from Classic Mac to Unix (LF)!!!
      goAnnotations="gene_annotation_only.tab" # two-column, tab-delimited, one line per gene, multiple GO terms separated by semicolon. If you have multiple lines per gene, use nrify_GOtable.pl prior to running this script.
      goDatabase="go.obo" # download from http://www.geneontology.org/GO.downloads.ontology.shtml
      goDivision="BP" # either MF, or BP, or CC
      source("gomwu.functions.R")

      # Calculating stats. It takes ~3 min for MF and BP. Do not rerun it if you just want to replot the data with different cutoffs, go straight to gomwuPlot. If you change any of the numeric values below, delete the files that were generated in previos runs first.
      gomwuStats(input, goDatabase, goAnnotations, goDivision, 
      perlPath="perl", # replace with full path to perl executable if it is not in your system's PATH already
      largest=0.1,  # a GO category will not be considered if it contains more than this fraction of the total number of genes
      smallest=5,   # a GO category should contain at least this many genes to be considered
      clusterCutHeight=0.25, # threshold for merging similar (gene-sharing) terms. 
      Alternative="g" # by default the MWU test is two-tailed; specify "g" or "l" of you want to test for "greater" or "less" instead
      )


------
<div id='id-section8'/> 
### Page 8: 2018-02-21 Continuing with DESEq2 and also clustering/module ID with WGCNA

Today, we looked at differential expession among all factors by binding them into groups.

        #this creates a new column in dataset that combines the different factors
        colData$group = factor(paste0(colData$population, "-", colData$devstage, "-", colData$sex))
        head(colData)
        dds = DESeqDataSetFromMatrix(countData = countData, colData = colData, design = ~ group)
        # DEseq2 interprets models as testing differences b/t last predictor accounting for previous predictors
        dds1 = dds[rowSums(counts(dds)) > 1, ]  #revises dds to exclude transcripts with 0 reads, as previous analysis included all transcripts
        dim(dds1) #check 
        dds1 = DESeq(dds1, parallel=T) #runs on more than one processor, if computer setup to do so
        resultsNames(dds1)
        
        res_pop_PP1_F = results(dds1, contrast = list(c("groupNC.PP1.F"), c("groupWA.PP1.F")), listValues=c(1/2, -1/2), alpha=0.05) #pulling out results for a particular comparison
        res_pop_PP1_F = res_pop_PP1_F[order(res_pop_PP1_F$padj),] #order by p-value
        head(res_pop_PP1_F)

        sig_pop_PP1_F = res_pop_PP1_F[which(res_pop_PP1_F$padj <0.05), ] #pull out sig. ones, can be used to generally subset data
        dim(sig_pop_PP1_F) #check number
        
        sig_pop_PP1_F_df = as.data.frame(sig_pop_PP1_F) #turns results into dataframe to use in heatmap plot
        sig_pop_PP1_F_df$Row.names = rownames(sig_pop_PP1_F_df)
        dim(sig_pop_PP1_F_df) 
        
        genesOfInterest_pop_PP1_F = c(sig_pop_PP1_F_df$Row.names) #pulls out gene names
        length(genesOfInterest_pop_PP1_F)  #check number
        
        vsd = vst(dds1, blind=F)
        dds1$combined = factor(paste0(dds1$population, "-", dds1$devstage, "-", dds1$sex))
        dds1$combined <- factor(dds1$combined, levels=c("WA-L3L-F","WA-L3L-M","WA-PP1-F","WA-PP1-M","WA-PD1-F","WA-PD1-M","WA-AD4-F","WA-AD4-M","NC-L3L-F","NC-L3L-M","NC-PP1-F","NC-PP1-M","NC-PD1-F","NC-PD1-M","NC-AD4-F","NC-AD4-M"), labels=c("WA-L3L-F","WA-L3L-M","WA-PP1-F","WA-PP1-M","WA-PD1-F","WA-PD1-M","WA-AD4-F","WA-AD4-M","NC-L3L-F","NC-L3L-M","NC-PP1-F","NC-PP1-M","NC-PD1-F","NC-PD1-M","NC-AD4-F","NC-AD4-M"))

        baseMeanPerGrp <- sapply( levels(dds1$combined), function(lvl) rowMeans(counts(dds1,normalized=TRUE)[,dds1$combined == lvl] ) )

        head(baseMeanPerGrp)
        dim(baseMeanPerGrp)
        
        #pulls out normalized counts (mean of 3 reps) for all the sig. genes, so we can put them on the same plot
        m = baseMeanPerGrp[genesOfInterest_pop_PP1_F, c("WA-PP1-F", "WA-PP1-M", "NC-PP1-F", "NC-PP1-M")]
        mat_scaled = t(apply(m, 1, scale))
        head(mat_scaled)
        
        library("RColorBrewer")
        library("pheatmap")
        
        pheatmap(mat_scaled, labels_col=c("WA-PP1-F", "WA-PP1-M", "NC-PP1-F", "NC-PP1-M"), cluster_cols = F, cluster_rows = T)
        
        norm.counts = counts(dds1, normalized=T)
        dim(norm.counts)
        write.csv(norm.counts, file="beetle_norm_counts.csv", row.names=T, quote=F)
        
We also started examining clustering analyses with WGCNA.

        source("http://bioconductor.org/biocLite.R") 
        biocLite(c("AnnotationDbi", "impute", "GO.db", "preprocessCore")) 
        install.packages("WGCNA")
        library("WGCNA")
        
        beetData = read.csv("beetle_norm_counts.csv")
        dim(beetData)
        Head(beetData)
        names(beetData)

        bdatExpr0 = as.data.frame(t(beetData[, -c(1:1)]))
        dim(bdatExpr0)
        names(bdatExpr0) = beetData$X
        
        row.names(bdatExpr0) = names(beetData)[-c(1:1)]

        bgsg = goodSamplesGenes(bdatExpr0, verbose = 3)

        ###### Make a cluster tree to look for outliers #####
        sampleTree = hclust(dist(bdatExpr0), method="average")

        sizeGrWindow(12,9)
        par(cex= 0.6)
        par(mar=c(0,4,2,0))

        plot(sampleTree, main="Sampling clustering to detect outliers", sub="", xlab="", cex.lab=1.5, cex.axis=1.5, cex.main=2)

        ####Read in trait data #####
        btraitData = read.table("cols_data_noIT_num.txt", header=T)
        dim(btraitData)
        head(btraitData)

        ####Network construction####
        powers = c(c(1:10), seq(from=12, to=20, by=2))

        sft = pickSoftThreshold(bdatExpr0, powerVector = powers, verbose=5)
        
        ####Data visulaization####
        sizeGrWindow(9,5)
        par(mfrow = c(1,2))
        cex=0.9
        
        plot(sft$fitIndices[,1], -sign(sft$fitIndices[,3])*sft$fitIndices[,2], xlab="Soft threshold (power)", ylab="Scale Free Topology Model Fit, signed R^2", type="n", main=paste("Scale independence"))
        text(sft$fitIndices[,1], -sign(sft$fitIndices[,3]*sft$fitIndices[,2], labels=powers, cex=1, col="red"))
        abline(h=0.9, col="red")

        #plot mean connectivity of soft threshold power
        plot(sft$fitIndices[,1], sft$fitIndices[,5], xlab="Soft threshold (power)", ylab="Mean Connectivity", type="n", main=paste("Mean Connectivity"))
        text(sft$fitIndices[,1], sft$fitIndices[,5], labels=powers, cex=cex1, col="red")

        net = blockwiseModules(bdatExpr0, power=6, TOMType = "unsigned", minModuleSize = 30, reassignThreshold = 0, mergeCutHeight=0.25, numericLabels=T, pamRespectsDendro=F, saveTOMS=T, saveTOMFileBase="beetleTOM", verbose=3)

        sizeGrWindow(12,9)
        mergedColors = labels2colors(net$colors)
        plotDendroAndColors(net$dendrograms[[1]], mergedColors[net$blockGenes[[1]]], "Module colors", dendroLabels=F, hang=0.03, addGuide=T, guideHang=0.05)

        moduleLabels = net$colors
        moduleColors = labels2colors(net$colors)
        MEs = net$MEs
        geneTree = net$dendrograms[[1]]
        
        ######Define number of genes in samples
        nGenes = ncol(bdatExpr0)
        nSamples = nrow(bdatExpr0)

        MEs0 = moduleEigengenes(bdatExpr0, moduleColors)$eigengenes
        MEs = orderMEs(MEs0)
        moduleTraitCor = cor(MEs, btraitData, use="p")
        moduleTraitPvalue = corPvalueStudent(moduleTraitCor, nSamples)
        
        sizeGrWindow(10,6)
        textMatrix = paste(signif(moduleTraitCor, 2), "\n(", signif(moduleTraitPvalue, 1), ")", sep="")
        dim(textMatrix) = dim(moduleTraitCor)
        par(mar = c(6,8.5,3,3))
        labledHeatmap(Matrix=moduleTraitCor, xLabels=names(btraitData), yLabels=names(MEs), ySymbols = names(MEs), colorLabels=F, colors=greenWhiteRed(50), textMatrix=textMatrix, setStdMargins=F, cex.text=0.5, zlim=c(-1,1), main=paste("Module-trait relationships"))

------
<div id='id-section9'/>
### Page 9: 2018-03-05: Code for Assignment 1 analysis

Here is my code for examining DGE between North Carolina and Western Australia beetle populations:

        ######################################### Assess DGE ##################################

        setwd("~/Documents/Academic-Research/UVM/Courses/Ecological Genomics/Data/DGE/Counts")
        
        library("DESeq2")
        library("ggplot2")
        
        ### Import data ###

        countsTable = read.delim('allcountsdataRN_noIT.txt', header=T, stringsAsFactors = T, row.names=1)
        countData = as.matrix(countsTable)
        head(countData) #check
        
        conds = read.delim('cols_data_noIT.txt', header=T, stringsAsFactors = T, row.names=1)
        head(conds) #check
        colData = as.data.frame(conds)


        ### Model to compare populations, controlling for sex and dev stage ###
        
        #want to identify differences between populations, but controlling for other relevant factors
        
        dds_pop = DESeqDataSetFromMatrix(countData = countData, colData = colData, design = ~ devstage + sex + population)
        # DEseq2 interprets models as testing differences b/t last predictor accounting for previous predictors
        
        dim(dds_pop) #gives summary of input
        dds_pop1 = dds_pop[rowSums(counts(dds_pop)) > 1, ]  #revises dds to exclude transcripts with 0 reads, as previous analysis included all transcripts
        dim(dds_pop1) #gives updated summary

        dds_pop1 = DESeq(dds_pop1, parallel=T, modelMatrixType = "standard")
        
        res = results(dds_pop1)  #this aggregates output of DESeq command
        res = res[order(res$padj),] #this re-orders output by adjusted p-value
        res = res[order(res$log2FoldChange),] #this re-orders output by Fold Change
        head(res) #gives a peak at output
        
        summary(res) #default is p<0.1
        summary(res, alpha=0.05)
        
        write.csv(res, file="DGE_NCvsWA_pop.csv", row.names=T, quote=F) #this outputs results as csv file
        
        sig_res_pop = res[which(res$padj <0.05), ] #pull out sig. only
        write.csv(sig_res_pop, file="DGE_NCvsWA_pop_sig.csv", row.names=T, quote=F) 
        
        neglogpval = as.matrix(-log(res$pvalue))   #script for GO Mann-Whitney test requires this format
        res_negpval = cbind(row.names(res), neglogpval) #here using raw p-value
        colnames(res_negpval)=c("gene", "neglogpval")
        
        write.csv(res_negpval, file="DGE_NCvsWA_neglogpval.csv", row.names=F, quote=F)
        
        #MA plot to visualize global differential expression
        plotMA(res, main="NC vs. WA Global Expression Differences", alpha=0.05, ylim=c(-1.5,1.5))
        #abline(h=c(-1,1), col="blue", lwd = 2)
        
        #PCA to get a global view of gene expression patterns
        rld_pop <- rlog(dds_pop1, blind=FALSE) 
        data <- plotPCA(rld_opop, intgroup=c("population","devstage","sex"), returnData=TRUE)
        percentVar <- round(100 * attr(data, "percentVar"))
        
        ggplot(data, aes(PC1, PC2, color=population, shape=sex)) +
          geom_point(size=4, alpha=0.85) +
          xlab(paste0("PC1: ",percentVar[1],"% variance")) +
          ylab(paste0("PC2: ",percentVar[2],"% variance")) +
          theme_minimal()
        
        ggplot(data, aes(PC1, PC2, color=population, shape=devstage)) +
          geom_point(size=4, alpha=0.85) +
          xlab(paste0("PC1: ",percentVar[1],"% variance")) +
          ylab(paste0("PC2: ",percentVar[2],"% variance")) +
          theme_minimal()
        
        ggplot(data, aes(PC1, PC2, color=sex, shape=devstage)) +
          geom_point(size=4, alpha=0.85) +
          xlab(paste0("PC1: ",percentVar[1],"% variance")) +
          ylab(paste0("PC2: ",percentVar[2],"% variance")) +
          theme_minimal()


        ### Models to compare sex-specific expression across developmental stages ###
        
        #want to compare NC Female vs. WA Female and NC Male vs. WA Male
        
        colData$group = factor(paste0(colData$population, "-", colData$sex)) #creates group of population and sex, 'ignoring' dev stage
        head(colData)

        # population devstage sex ind group
        # NC_AD4_F1_         NC      AD4   F   1  NC-F
        # NC_AD4_F2_         NC      AD4   F   2  NC-F
        # NC_AD4_F3_         NC      AD4   F   3  NC-F
        # NC_AD4_M1_         NC      AD4   M   1  NC-M
        # NC_AD4_M2_         NC      AD4   M   2  NC-M
        # NC_AD4_M3_         NC      AD4   M   3  NC-M
        
        dds_sex = DESeqDataSetFromMatrix(countData = countData, colData = colData, design = ~ group)
        
        dim(dds_sex) 
        dds_sex1 = dds_sex[rowSums(counts(dds_sex)) > 1, ]  
        dim(dds_sex1) 
        
        dds_sex1 = DESeq(dds_sex1, parallel=T)
        
        resultsNames(dds_sex1)
        # [1] "Intercept" "groupNC.F" "groupNC.M" "groupWA.F" "groupWA.M"

        #For females
        res_F = results(dds_sex1, contrast = list(c("groupNC.F"), c("groupWA.F")), listValues=c(1/2, -1/2))
        res_F = res_F[order(res_F$padj),]
        head(res_F)
        
        summary(res_F)
        summary(res_F, alpha=0.05)
        
        write.csv(res_F, file="DGE_NCvsWA_F.csv", row.names=T, quote=F)
        
        sig_res_F = res_F[which(res_F$padj <0.05), ] #pull out sig. only
        write.csv(sig_res_F, file="DGE_NCvsWA_F_sig.csv", row.names=T, quote=F)

        #For males
        res_M = results(dds_sex1, contrast = list(c("groupNC.M"), c("groupWA.M")), listValues=c(1/2, -1/2))
        res_M = res_F[order(res_M$padj),]
        head(res_M)
        
        summary(res_M)
        summary(res_M, alpha=0.05)
        
        sig_res_M = res_M[which(res_M$padj <0.05), ] #pull out sig. only
        
        write.csv(res_M, file="DGE_NCvsWA_M.csv", row.names=T, quote=F)
        write.csv(sig_res_M, file="DGE_NCvsWA_M_sig.csv", row.names=T, quote=F)

        neglogpval_F = as.matrix(-log(res_F$pvalue))   #script for GO Mann-Whitney test requires this format
        res_negpval_F = cbind(row.names(res_F), neglogpval_F)
        colnames(res_negpval_F)=c("gene", "neglogpval")
        
        write.csv(res_negpval_F, file="DGE_NCvsWA_F_neglogpval.csv", row.names=F, quote=F)
        
        neglogpval_M = as.matrix(-log(res_M$pvalue))   #script for GO Mann-Whitney test requires this format
        res_negpval_M = cbind(row.names(res_M), neglogpval_M) #this uses raw p-value rather than adjusted (seems to be too restrictive for GO script)
        colnames(res_negpval_M)=c("gene", "neglogpval")
        
        write.csv(res_negpval_M, file="DGE_NCvsWA_M_neglogpval.csv", row.names=F, quote=F)

        #PCA to get a global view of gene expression
        rld_sex <- rlog(dds_sex1, blind=FALSE) 
        data <- plotPCA(rld_sex, intgroup=c("population","devstage","sex"), returnData=TRUE)
        percentVar <- round(100 * attr(data, "percentVar"))
        
        ggplot(data, aes(PC1, PC2, color=population, shape=sex)) +
          geom_point(size=4, alpha=0.85) +
          xlab(paste0("PC1: ",percentVar[1],"% variance")) +
          ylab(paste0("PC2: ",percentVar[2],"% variance")) +
          theme_minimal()
        #basically shows same patterns as above, not terribly informative

        #For heat map of within sex comparisons
        library("plyr") #for manipulating data frames
        library("dplyr")
        library("RColorBrewer")
        library("pheatmap")
        
        Pop_F_df = as.data.frame(res_F) #turns results into dataframe to use in heatmap plot
        Pop_F_df$Row.names = rownames(Pop_F_df)
        
        Pop_M_df = as.data.frame(res_M) #turns results into dataframe to use in heatmap plot
        Pop_M_df$Row.names = rownames(Pop_M_df)
        
        sig_pop_F = res_F[which(res_F$padj <0.05), ] #pull out sig. ones from F-F
        sig_pop_M = res_F[which(res_M$padj <0.05), ] #pull out sig. ones from M-M

        sig_pop_F_df = as.data.frame(sig_pop_F) #turns results into dataframe to use in heatmap plot
        sig_pop_F_df$Row.names = rownames(sig_pop_F_df)
        dim(sig_pop_F_df) 
        
        sig_pop_M_df = as.data.frame(sig_pop_M) #turns results into dataframe to use in heatmap plot
        sig_pop_M_df$Row.names = rownames(sig_pop_M_df)
        dim(sig_pop_M_df) 
        
        sig_pop_FM = rbind(sig_pop_F_df, sig_pop_M_df) #combine these into one data frame
        #sig_pop_FM1 = distinct(sig_pop_FM) #remove any duplicates
        dim(sig_pop_FM)
        
        genesOfInterest_pop_FM = c(sig_pop_FM$Row.names) #pulls out gene names
        length(genesOfInterest_pop_FM)

        vsd = vst(dds_sex1, blind=F)
        dds_sex1$combined = factor(paste0(dds_sex1$population, "-", dds_sex1$sex))
        dds_sex1$combined <- factor(dds_sex1$combined, levels=c("NC-F","NC-M","WA-F","WA-M"), labels=c("NC-F","NC-M","WA-F","WA-M"))
        
        baseMeanPerGrp <- sapply( levels(dds_sex1$combined), function(lvl) rowMeans(counts(dds_sex1,normalized=TRUE)[,dds_sex1$combined == lvl] ) )
        
        head(baseMeanPerGrp) #check
        dim(baseMeanPerGrp)

        #pulls out normalized counts (mean of 3 reps) for all the sig. genes, so we can put them on the same plot
        m = baseMeanPerGrp[genesOfInterest_pop_FM, c("NC-F","NC-M","WA-F","WA-M")]
        
        mat_scaled = t(apply(m, 1, scale))
        
        pheatmap(mat_scaled, labels_col=c("NC-F", "NC-M", "WA-F", "WA-M"), cluster_cols = F, cluster_rows = T)
        
        
        ################################# Examine GO Enrichment ###############################

        ### Mann-Whitney Tests for GO Analysis ###
        
        setwd("~/Documents/Academic-Research/UVM/Courses/Ecological Genomics/Data/DGE/Enrichment")
        library(ape)
        
        
        # Importing data:

        input="DGE_NCvsWA_neglogpval.csv" # two columns of comma-separated values: gene id, continuous measure of significance. To perform standard GO enrichment analysis based on Fisher's exact test, use binary measure (0 or 1, i.e., either sgnificant or not).
        ## Make sure this is saved as Unix (LF) - open in TextWrangler, save as, change from Classic Mac to Unix (LF)!!!
        
        input="DGE_NCvsWA_F_neglogpval.csv"
        
        input="DGE_NCvsWA_M_neglogpval.csv"
        
        goAnnotations="gene_annotation_only.tab" # two-column, tab-delimited, one line per gene, multiple GO terms separated by semicolon. If you have multiple lines per gene, use nrify_GOtable.pl prior to running this script.
        goDatabase="go.obo" # download from http://www.geneontology.org/GO.downloads.ontology.shtml
        goDivision="BP" # either MF, or BP, or CC
        source("gomwu.functions.R")

        
        # Calculating stats. It takes ~3 min for MF and BP. Do not rerun it if you just want to replot the data with different cutoffs, go straight to gomwuPlot. If you change any of the numeric values below, delete the files that were generated in previous runs first.
        gomwuStats(input, goDatabase, goAnnotations, goDivision,
                   perlPath="perl", # replace with full path to perl executable if it is not in your system's PATH already
                   largest=0.1,  # a GO category will not be considered if it contains more than this fraction of the total number of genes
                   smallest=5,   # a GO category should contain at least this many genes to be considered
                   clusterCutHeight=0.25, # threshold for merging similar (gene-sharing) terms. 
                   Alternative= "g" # by default the MWU test is two-tailed; specify "g" or "l" of you want to test for "greater" or "less" instead
        )

        quartz() #creates new window for cluster tree
        gomwuPlot(input,goAnnotations,goDivision,
                  absValue=-log(0.05,10),  # genes with the measure value exceeding this will be counted as "good genes". Specify absValue=0.5 if you are doing Fisher's exact test for standard GO enrichment.
                  level1=0.1, # FDR threshold for plotting. Specify level1=1 to plot all GO categories containing genes exceeding the absValue.
                  level2=0.05, # FDR cutoff to print in regular (not italic) font.
                  level3=0.01, # FDR cutoff to print in large bold font.
                  txtsize=1.2,    # decrease to fit more on one page, or increase (after rescaling the plot so the tree fits the text) for better "word cloud" effect
                  treeHeight=0.5, # height of the hierarchical clustering tree
                  colors=c("dodgerblue2","firebrick1","skyblue","lightcoral") # these are default colors, un-remar and change if needed
        )
        
        
------
<div id='id-section10'/>
### Page 10: 2018-02-26 and 2018-02-28: Intro to population genomics

I actually missed class this week as I was in Amherst, but here's my followup.

First, we need to convert our data in .sam format to .bam format, which is the binary version. Then we can sort it, remove unpaired reads, remove PCR duplicates, and index. Make this as a bash script called CallSNPs.sh:

        #!/bin/bash

        samtools view -@ 4 -bS WA_PP1_F1_bwamem.sam > WA_PP1_F1_bwamem.bam #convert to bam
        samtools flagstat WA_PP1_F1_bwamem.bam #gives summary
        samtools depth WA_PP1_F1_bwamem.bam #seq depth
        samtools fixmate -@ 4 WA_PP1_F1_bwamem.bam WA_PP1_F1_bwamem.fm.bam #remove unpaired
        samtools sort -@ 4 WA_PP1_F1_bwamem.fm.bam -o WA_PP1_F1_bwamem.fm.sort.bam #sort reads
        samtools rmdup WA_PP1_F1_bwamem.fm.sort.bam WA_PP1_F1_bwamem.fm.sort.rmdup.bam #remove PCR duplicates
        samtools index WA_PP1_F1_bwamem.fm.sort.bam #indexes file to easier computations
        
Once done, we want to look at our data before proceeding:

        samtools tview WA_PP1_F1_bwamem.fm.sort.bam

Next, we want to map our reads to the reference, which we do my adding to our CallSNPs.sh

        bcftools mpileup -Ou -f /data/project_data/beetles/reference/OTAU.fna ~/mydata/WA_PP1_F1_bwamem.fm.sorted.bam --threads 4 --skip-indel --annotate AD,DP | bcftools call -Ov -mv --format-fields GQ,GP > WA_PP1_F1.vcf
        

------
<div id='id-section11'/>
### Page 11: 2018-03-05: Continuing with working with vcf files

Today, we compared different methods to identify SNPs from the beetle RNAseq data.

        cd /data/project_data/beetles/snps
        
Using vcftools to process the vcf file to retain only biallelic SNPs, remove SNPs below 0.01 freq, retain only high quality SNPs, remove SNPs with >80% missing data, output heterozygosity measure:

        vcftools --vcf OTAU_2018_samtools.vcf --min-alleles 2 --max-alleles 2 --maf 0.01 --minGQ 20 --max-missing 0.8 --het --out ~/mydata/samtools0.8
        #output: Parameters as interpreted:
        	--vcf OTAU_2018_samtools.vcf
        	--maf 0.01
        	--max-alleles 2
        	--min-alleles 2
        	--minGQ 20
        	--max-missing 0.8
        	--het
        	--out /users/j/k/jkostyun/mydata/samtools0.8
        
        After filtering, kept 72 out of 72 Individuals
        Outputting Individual Heterozygosity
        After filtering, kept 117 out of a possible 611928 Sites
        Run Time = 19.00 seconds
        
Testing how this differs by changing the quality requirements:

        vcftools --vcf OTAU_2018_samtools.vcf --min-alleles 2 --max-alleles 2 --maf 0.01 --minGQ 10 --max-missing 0.8 --het --out ~/mydata/samtools0.8v2
        #output: Parameters as interpreted:
        	--vcf OTAU_2018_samtools.vcf
        	--maf 0.01
        	--max-alleles 2
        	--min-alleles 2
        	--minGQ 10
        	--max-missing 0.8
        	--het
        	--out /users/j/k/jkostyun/mydata/samtools0.8v2
        
        After filtering, kept 72 out of 72 Individuals
        Outputting Individual Heterozygosity
        After filtering, kept 83120 out of a possible 611928 Sites
        Run Time = 20.00 seconds
        
Next we examined the inbreeding coefficient as another way to assess retention of SNPs. In R:

        df=read.table("samtools0.8v2.het", header=T)
        str(df)
        #output: 
        'data.frame':	72 obs. of  5 variables:
         $ INDV   : Factor w/ 72 levels "IT_AD4_F1_","IT_AD4_F2_",..: 1 2 3 4 5 6 8 9 10 7 ...
         $ O.HOM. : int  66334 59874 69153 61780 67300 69025 64488 73824 68621 67290 ...
         $ E.HOM. : num  66365 60720 69302 61431 67564 ...
         $ N_SITES: int  70922 64890 74059 65648 72209 73834 70006 81319 75543 73303 ...
         $ F      : num  -0.00683 -0.20302 -0.03141 0.08282 -0.05684 ...
         summary(df$F)
         #output:
         Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
    -0.748830 -0.193505  0.003465 -0.015433  0.163995  0.320880 
        sd(df$F) #0.2154291
        
Back to command-line. As a class, we determined that using the read2snps method and max missing data 0.8 works best:

      vcftools --vcf OTAU_2018_reads2snps_DP10GP95.vcf --min-alleles 2 --max-alleles 2 --maf 0.01  --max-missing 0.8 --recode --out ~/mydata/OTAU_2018_reads2snps_DP10GP95_biallelic_MAF0.01_Miss0.8.vcf
      #output: Parameters as interpreted:
      	--vcf OTAU_2018_reads2snps_DP10GP95.vcf
      	--maf 0.01
      	--max-alleles 2
      	--min-alleles 2
      	--max-missing 0.8
      	--out /users/j/k/jkostyun/mydata/OTAU_2018_reads2snps_DP10GP95_biallelic_MAF0.01_Miss0.8.vcf
      	--recode
      
      After filtering, kept 72 out of 72 Individuals
      Outputting VCF file...
      After filtering, kept 133291 out of a possible 33366006 Sites
      Run Time = 89.00 seconds
      
      grep "IT" /data/project_data/beetles/metadata/cols_data.txt #find IT data
      grep "IT" /data/project_data/beetles/metadata/cols_data.txt | cut -f 1 >IT.inds #pull out IT data and create new file
      vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF0.01_Miss0.8.vcf.recode.vcf --keep IT.inds --freq2 --out IT
      #output: Parameters as interpreted:
        	--vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF0.01_Miss0.8.vcf.recode.vcf
        	--keep IT.inds
        	--freq2
        	--out IT
        
        Keeping individuals in 'keep' list
        After filtering, kept 24 out of 72 Individuals
        Outputting Frequency Statistics...
        After filtering, kept 133291 out of a possible 133291 Sites
        Run Time = 2.00 seconds
        
        head IT.frq #check file
        
------
<div id='id-section12'/>
### Page 12: 2018-05-07: Continuing with exploring allele frequencies

Today, we continued exploring the beetle RNAseq data with respect to SNPs by pulling out data for the remaining populations.

        grep "NC" /data/project_data/beetles/metadata/cols_data.txt | cut -f 1 >NC.inds
        vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF0.01_Miss0.8.vcf.recode.vcf --keep NC.inds --freq2 --out NC
        grep "WA" /data/project_data/beetles/metadata/cols_data.txt | cut -f 1 >WA.inds
        vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF0.01_Miss0.8.vcf.recode.vcf --keep WA.inds --freq2 --out WA
        #want to remove SNPs that are linked, so here retaining only one SNP per 1000bp
        vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF0.01_Miss0.8.vcf.recode.vcf --thin 1000 --recode

After getting allele frequencies for each population separately, we visualized minor allele frequency spectrum plots in R.

        IT = read.table("IT.frq", skip=1, header=F) 
        head(IT) #check
        dim(IT) #check size
        #lists gene name, bp position of SNP in gene, how many alelles at SNP, number of alleles sampled in population, reference allele freq, minor allele freq
        
        NC = read.table("NC.frq", skip=1, header=F)
        WA = read.table("WA.frq", skip=1, header=F)
        
        #hist(IT$V6, col="red", main="MAF in IT Population", xlab="Minor Allele")
        
        IT = hist(IT$V6, plot=F)
        IT$counts = IT$counts/sum(IT$counts)
        plot(IT$counts, type="l", main="Normalized MAF in IT Population")
        
        NC = hist(NC$V6, plot=F)
        NC$counts = NC$counts/sum(NC$counts)
        plot(NC$counts, type="l", main="Normalized MAF in NC Population")
        
        WA = hist(WA$V6, plot=F)
        WA$counts = WA$counts/sum(WA$counts)
        plot(WA$counts, type="l", main="Normalized MAF in WA Population")
        
        #plot all together
        plot(IT$counts, type="l", col="red", lwd=1.5, ylim=c(0,0.5), main="Normalized MAF among Populations")
        lines(NC$counts, col="blue", lwd=1.5)
        lines(WA$counts, col="green", lwd=1.5)
        
Next we copied files for ADMIXTURE analysis to our working directories:

        cp /data/project_data/beetles/metadata/beetles.pop .
        cp /data/scripts/beetle.spid .
        cp /data/scripts/vcf2geno.sh .
        cp /data/project_data/beetles/snps/out.recode.vcf.geno .
        
And created a bash script to run ADMIXTURE for K= 1 - 10:

        vim #to create new bash script
        
This is what it looked like:   

        #!/bin/bash
        for K in {1...10}
        do
        admixture -j 4 --cv 10 ~/mydata/out.recode.vcf.geno $K | tee log${K}.out
        done
        grep "CV" log*out >chooseK.txt
        
        
        chmod u+x ADMIX.sh #to change file permissions

------
<div id='id-section13'/>
### Page 13: 2018-03-19: Using SNP data to examine population structure

Today, we continued getting our SNP data ready to use in ADMIXTURE to examine population structure. This included slightly revising our vcf2geno.sh script, which calls to the PGDSpider program to convert our SNP data in vcf format to geno format which is required by ADMIXTURE:

       vim vcf2geno.sh #to edit script to look like this:
       
        #!/bin/bash

        java -Xmx2G -jar /data/popgen/PGDSpider_2.0.9.0/PGDSpider2-cli.jar -inputfile ~/mydata/SNPs/out.recode.vcf -inputformat VCF -outputfile ~/mydata/SNPs/out.recode.vcf.geno -outputformat EIGENSOFT -spid ~/myscripts/beetle.spid
        
This output format codes SNPs as: 0=homozygous ref, 1=heterozgous, 2=homozygous derived, 9=missing data. 

Next, we slightly updated our ADMIX.sh script to make it actually run. Here, the cv refers to cross validation, where the program divides the data into n groups. It then cycles though, masking one group at a time: it trains on the 'available' data, and then tests its assignment model with the masked group. Well supported estimates of K will return lower values of cv error (i.e. the model is better at predicting the assignment of the masked group).
       
       vim ADMIX.sh   
       
       #!/bin/bash

        for K in {1..10}
        
        do
        
        admixture -j4 --cv=10 ~/mydata/SNPs/out.recode.vcf.geno $K | tee log${K}.out
        
        done
        
        grep "CV" log*out >chooseK.txt
       
The output from ADMIX.sh includes multiple types of files: .P includes allele freq for each K, .Q provides ancestry coeff for each K, and .log is logged output for each K. It then also searches within the log files to find the cv error for eack value of K, and puts this into the chooseK.txt file. Here is what that file looked like:

        cat chooseK.txt
        
        log10.out:CV error (K=10): 0.74078
        log1.out:CV error (K=1): 0.44089
        log2.out:CV error (K=2): 0.43635
        log3.out:CV error (K=3): 0.44473
        log4.out:CV error (K=4): 0.47987
        log5.out:CV error (K=5): 0.51791
        log6.out:CV error (K=6): 0.55534
        log7.out:CV error (K=7): 0.58280
        log8.out:CV error (K=8): 0.62599
        log9.out:CV error (K=9): 0.66978
        
Based on these cv errors, we want to focus on K=2 through K=4. We therefore moved the .Q files to our local computer so as to be able to visualize the data using the R package pophelper:

        install.packages(c("Cairo","ggplot2","gridExtra","gtable","tidyr","devtools"),dependencies=T)
        
However, I needed to install an earlier version of Cairo than the one that is automatically called to in the above install command, as well as install X11 to properly load the Cairo package. Once I did that, I could install and load in the pophelper package:

        devtools::install_github('royfrancis/pophelper')
        
We also moved cols_data.text, which contains metadata for all the beetle individuals, to our local computers for use in R.

        setwd("~/Documents/Academic-Research/UVM/Courses/Ecological Genomics/Data/Pop_Genomics")

        library(pophelper)
        
        admixfiles=list.files(path=("ADMIXTURE/"),full.names=T)
        admixlist=readQ(files=admixfiles,filetype="basic")
        
        # metadata: sample id and pop from beetle metadata file
        metadata=read.table("cols_data.txt",header=T)
        
        # format metadata to a data frame and ind variables as chars. for plotting
        metadata2=data.frame(sampleid=metadata[,1], population=metadata[,2])
        
        metadata2$sampleid=as.character(metadata2$sampleid)
        metadata2$population=as.character(metadata2$population)
        
        # add in the sample id to the different Q files for plotting
        if(length(unique(sapply(admixlist,nrow)))==1)
          admixlist <- lapply(admixlist,"rownames<-",metadata2$sampleid)
        
        head(admixlist[[3]]) #peak at what the ancestry assignments for K=4 look like (K=4 is in 3rd column in this datafile)
        
        p <- plotQ(admixlist[c(1,2,3)],
                   returnplot=T,exportplot=T,quiet=T,basesize=11, imgoutput="join", 
                   showyaxis=T, showticks=T, panelspacer=0.4, useindlab=F, showindlab=F, 
                   grplab=metadata2[2], grplabsize=4, linesize=1, barsize=1, pointsize=3, 
                   panelratio=c(4,1), divsize = 0.75, pointcol="white", showtitle=T, 
                   titlelab="ADMIXTURE analysis on O. tauri, SNPs thinned to 1 per kb", 
                   splab=c("K=2","K=3","K=4"), outputfilename="ADMIXTURE_Otauri",
                   imgtype="pdf",height=3,width=25)
        
        plot(p$plot[[1]])


------
<div id='id-section14'/>
### Page 14: 2018-03-21: Using Bayescan to identify loci under differential selection

Today, we discussed various approaches to identifying loci that might be under differential selection in different populations. A common way to do so is to find Fst outliers, however, Fst is also highly dependent on population structure and demographic history. We therefore used a method called Bayescan which explictly incorporates a demographic model to determine expected Fst values given that particular population structure and history (parameter beta in model). It also determines parameter alpha, which is a differentiation metric for loci which incorporates the expected values of beta. Alpha therefore approximates the amount of differentiation between populations due to selection, rather than drift/demography. Alpha values around 0 indicate neutrality, highly negative values indicate purifying or stabilizing selection (i.e. populations are more similar to one another than expected given their demographic model), and highly positive values indicate directional selection (i.e. populations are more differentiated than expected under the given demographic model).

To run Bayescan on the beetle data, we first need to convert our .vcf SNP file to the bayescan format. This can be done using PGDSpider. However, because this takes a long time, the converted file was provided to us:

        /data/project_data/beetles/snps/OTAU_2018_reads2snps_DP10GP95_biallelic_MAF01_Miss0.8.vcf.recode.vcf.bayescan
        
To actually run Bayescan, we created a bash script:

        vim #to create new, this is what it looked like:
        
        #!/bin/bash
        
        for thin in 10 20 30   #here, we are running different MCMC chains and sampling every 10, 20, or 30 values

        do
        
        bayescan /data/project_data/beetles/snps/OTAU_2018_reads2snps_DP10GP95_biallelic_MAF01_Miss0.8.vcf.recode.vcf.bayescan -od ~/mydata/SNPs -threads 4 -pr_odds 1000 -thin $thin -o $thin
        
        #the above command tells it to run Bayescan on that data file, to output to mydata/SNPs folder, to use 4 threads of computing power, to use 1000 for the prior (means the model assumes every 1 in 1000 is under selection), to use the above specified values of $thin, and to output each run to a different filename
        
        done
        
To save and exit the script:

        :w Bayescan.sh
        :q
        chmod u+x Bayescan.sh #to change file permissions
        
To then run the script, which will take a long time so we used screen:

        screen
        bash Bayescan.sh
        "control A+D" #to detach
        


------
<div id='id-section15'/>
### Page 15: 2018-03-22: Code for Assignment 2 analysis

To begin with, I used the OTAU_2018_reads2snps_DP10GP95_biallelic_MAF0.01_Miss0.8.vcf.recode.vcf file that we had previously generated as a class. It contains beetle SNPs that were filtered to be biallelic, rare alleles under 0.01 freq were removed, and sites with more than 80% missing data were removed.

From here, I created a new bash script to further filter this file using the --thin option, which allows us to set how many SNPs per given region size are retained (related to linkage). Here is that script:

        #!/bin/bash
        
        
        for thin in 10 100 1000 10000
        do
        vcftools --vcf ~/mydata/SNPs/OTAU_2018_reads2snps_DP10GP95_biallelic_MAF0.01_Miss0.8.vcf.recode.vcf --thin $thin --recode --out ~/mydata/SNPs/OTAU_2018_reads2SNPs${thin}
        done
        
Next, I converted each of these .vcf files to .geno format to use in ADMIXTURE, by editing our vcf2geno.sh script. I wasn't entirely sure how to specify the different input and output files, so I just repeated the command:

        java -Xmx2G -jar /data/popgen/PGDSpider_2.0.9.0/PGDSpider2-cli.jar -inputfile ~/mydata/SNPs/OTAU_2018_reads2SNPs10.recode.vcf -inputformat VCF -outputfile ~/mydata/SNPs/OTAU_2018_reads2SNPs10.recode.geno -outputformat EIGENSOFT -spid ~/myscripts/beetle.spid

        java -Xmx2G -jar /data/popgen/PGDSpider_2.0.9.0/PGDSpider2-cli.jar -inputfile ~/mydata/SNPs/OTAU_2018_reads2SNPs100.recode.vcf -inputformat VCF -outputfile ~/mydata/SNPs/OTAU_2018_reads2SNPs100.recode.geno -outputformat EIGENSOFT -spid ~/myscripts/beetle.spid
        
        java -Xmx2G -jar /data/popgen/PGDSpider_2.0.9.0/PGDSpider2-cli.jar -inputfile ~/mydata/SNPs/OTAU_2018_reads2SNPs1000.recode.vcf -inputformat VCF -outputfile ~/mydata/SNPs/OTAU_2018_reads2SNPs1000.recode.geno -outputformat EIGENSOFT -spid ~/myscripts/beetle.spid
        
        java -Xmx2G -jar /data/popgen/PGDSpider_2.0.9.0/PGDSpider2-cli.jar -inputfile ~/mydata/SNPs/OTAU_2018_reads2SNPs10000.recode.vcf -inputformat VCF -outputfile ~/mydata/SNPs/OTAU_2018_reads2SNPs10000.recode.geno -outputformat EIGENSOFT -spid ~/myscripts/beetle.spid


From here, I edited our ADMIX.sh script (saved as ADMIX2.sh) to estimate cv errors for each of these .vcf files. Again, I wasn't sure how to correctly specify more than one input file, so I just repeated the command. And I made sure to edit the log file names, so the grep function would summarize from the correct files:

        #!bin/bash
        
        for K in {1..10}
        
        do
        
        admixture -j4 --cv=10 ~/mydata/SNPs/OTAU_2018_reads2SNPs10.recode.geno $K | tee log10_${K}.out
        
        done
        
        grep "CV" log10*out >chooseK_10.txt
        
        for K in {1..10}
        
        do
        
        admixture -j4 --cv=10 ~/mydata/SNPs/OTAU_2018_reads2SNPs100.recode.geno $K | tee log100_${K}.out
        
        done
        
        grep "CV" log100*out >chooseK_100.txt
        
        
        for K in {1..10}
        
        do
        
        admixture -j4 --cv=10 ~/mydata/SNPs/OTAU_2018_reads2SNPs1000.recode.geno $K | tee log1000_${K}.out
        
        done
        
        grep "CV" log1000*out >chooseK_1000.txt
        
        for K in {1..10}
        
        do
        
        admixture -j4 --cv=10 ~/mydata/SNPs/OTAU_2018_reads2SNPs10000.recode.geno $K | tee log10000_${K}.out
        
        done
        
        grep "CV" log10000*out >chooseK_10000.txt
        
        
Here are the chooseK files:

        cat chooseK_10.txt
        
        log10_10.out:CV error (K=10): 0.74766
        log10_1.out:CV error (K=1): 0.45123   #good
        log10_2.out:CV error (K=2): 0.44376   #best
        log10_3.out:CV error (K=3): 0.45275   #good
        log10_4.out:CV error (K=4): 0.48883   #good
        log10_5.out:CV error (K=5): 0.51712
        log10_6.out:CV error (K=6): 0.56384
        log10_7.out:CV error (K=7): 0.59017
        log10_8.out:CV error (K=8): 0.63056
        log10_9.out:CV error (K=9): 0.65263
        
        cat chooseK_100.txt
        
        log100_10.out:CV error (K=10): 0.75042
        log100_1.out:CV error (K=1): 0.45050   #good
        log100_2.out:CV error (K=2): 0.44345   #best
        log100_3.out:CV error (K=3): 0.45297   #good
        log100_4.out:CV error (K=4): 0.48985   #good
        log100_5.out:CV error (K=5): 0.51799
        log100_6.out:CV error (K=6): 0.55990
        log100_7.out:CV error (K=7): 0.59106
        log100_8.out:CV error (K=8): 0.63564
        log100_9.out:CV error (K=9): 0.65572
        
        cat chooseK_1000.txt
        
        log1000_10.out:CV error (K=10): 0.74078
        log1000_1.out:CV error (K=1): 0.44089   #good
        log1000_2.out:CV error (K=2): 0.43635   #best
        log1000_3.out:CV error (K=3): 0.44470   #good
        log1000_4.out:CV error (K=4): 0.47987   #good
        log1000_5.out:CV error (K=5): 0.51791
        log1000_6.out:CV error (K=6): 0.55534
        log1000_7.out:CV error (K=7): 0.58280
        log1000_8.out:CV error (K=8): 0.62599
        log1000_9.out:CV error (K=9): 0.66978
        
        cat chooseK_10000.txt
        
        log10000_10.out:CV error (K=10): 0.74706
        log10000_1.out:CV error (K=1): 0.43924   #good
        log10000_2.out:CV error (K=2): 0.43462   #best
        log10000_3.out:CV error (K=3): 0.44418   #good
        log10000_4.out:CV error (K=4): 0.48223   #good
        log10000_5.out:CV error (K=5): 0.51968
        log10000_6.out:CV error (K=6): 0.55327
        log10000_7.out:CV error (K=7): 0.59704
        log10000_8.out:CV error (K=8): 0.62966
        log10000_9.out:CV error (K=9): 0.65378

Based on these results, K is between 1 and 4, with K=2 the best estimate. I therefore moved the .Q files for K=2 through K=4 to my local directory to use in R package pophelper (K=1 is literally one group, so I disreguarded it for this exercise). 

        library(pophelper)
        
        admixfiles=list.files(path=("ADMIXTURE2/"),full.names=T) #in ADMIXTURE2, I put the .Q files for K=2, K=3, and K=4 from each of the thin runs: 10, 100, 1000, and 10000
        admixlist=readQ(files=admixfiles,filetype="basic")

        # metadata: sample id and pop from beetle.pop file
        metadata=read.table("cols_data.txt",header=T)
        
        # format metadata to a data frame and ind variables as chars. for plotting
        metadata2=data.frame(sampleid=metadata[,1], population=metadata[,2])
        
        metadata2$sampleid=as.character(metadata2$sampleid)
        metadata2$population=as.character(metadata2$population)
        
        # add in the sample id to the different Q files for plotting
        if(length(unique(sapply(admixlist,nrow)))==1)
          admixlist <- lapply(admixlist,"rownames<-",metadata2$sampleid)
        
        head(admixlist[[3]]) #peak at what the ancestry assignments for K=4 look like (K=4 is in 3rd column in this datafile)
        
        p.10 <- plotQ(admixlist[c(1,2,3)],     #plot for 1 SNP in 10bp
                   returnplot=T,exportplot=T,quiet=T,basesize=11, imgoutput="join", 
                   showyaxis=T, showticks=T, panelspacer=0.4, useindlab=F, showindlab=F, 
                   grplab=metadata2[2], grplabsize=4, linesize=1, barsize=1, pointsize=3, 
                   panelratio=c(4,1), divsize = 0.75, pointcol="white", showtitle=T, 
                   titlelab="ADMIXTURE analysis on O. tauri, SNPs 1 in 10bp", 
                   splab=c("K=2","K=3","K=4"), outputfilename="ADMIXTURE_Otauri_10",
                   imgtype="pdf",height=3,width=15)
        
        plot(p.10$plot[[1]])
        
        p.100 <- plotQ(admixlist[c(4,5,6)],    #plot for 1 SNP in 100bp
                   returnplot=T,exportplot=T,quiet=T,basesize=11, imgoutput="join", 
                   showyaxis=T, showticks=T, panelspacer=0.4, useindlab=F, showindlab=F, 
                   grplab=metadata2[2], grplabsize=4, linesize=1, barsize=1, pointsize=3, 
                   panelratio=c(4,1), divsize = 0.75, pointcol="white", showtitle=T, 
                   titlelab="ADMIXTURE analysis on O. tauri, SNPs 1 in 100bp", 
                   splab=c("K=2","K=3","K=4"), outputfilename="ADMIXTURE_Otauri_100",
                   imgtype="pdf",height=3,width=15)
        
        plot(p.100$plot[[1]])
        
        p.1000 <- plotQ(admixlist[c(7,8,9)],    #plot for 1 SNP in 1Kb
                   returnplot=T,exportplot=T,quiet=T,basesize=11, imgoutput="join", 
                   showyaxis=T, showticks=T, panelspacer=0.4, useindlab=F, showindlab=F, 
                   grplab=metadata2[2], grplabsize=4, linesize=1, barsize=1, pointsize=3, 
                   panelratio=c(4,1), divsize = 0.75, pointcol="white", showtitle=T, 
                   titlelab="ADMIXTURE analysis on O. tauri, SNPs 1 in 1kb", 
                   splab=c("K=2","K=3","K=4"), outputfilename="ADMIXTURE_Otauri_1000",
                   imgtype="pdf",height=3,width=15)
        
        plot(p.1000$plot[[1]])
        
        p.10000 <- plotQ(admixlist[c(10,11,12)],   #plot for 1 SNP in 10Kb
                   returnplot=T,exportplot=T,quiet=T,basesize=11, imgoutput="join", 
                   showyaxis=T, showticks=T, panelspacer=0.4, useindlab=F, showindlab=F, 
                   grplab=metadata2[2], grplabsize=4, linesize=1, barsize=1, pointsize=3, 
                   panelratio=c(4,1), divsize = 0.75, pointcol="white", showtitle=T, 
                   titlelab="ADMIXTURE analysis on O. tauri, SNPs 1 in 10kb", 
                   splab=c("K=2","K=3","K=4"), outputfilename="ADMIXTURE_Otauri_10000",
                   imgtype="pdf",height=3,width=15)
        
        plot(p.10000$plot[[1]])

To examine pi, I first specified each population:

        grep "IT" /data/project_data/beetles/metadata/cols_data.txt | cut -f 1 >IT.inds 
        grep "NC" /data/project_data/beetles/metadata/cols_data.txt | cut -f 1 >NC.inds 
        grep "WA" /data/project_data/beetles/metadata/cols_data.txt | cut -f 1 >WA.inds 
        
Then I calculated pi for each site for each population using vcftools, and transferred the output files to my local directory to get summary statistics.
        
        vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF0.01_Miss0.8.vcf.recode.vcf --keep IT.inds --site-pi --out IT.pi
        vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF0.01_Miss0.8.vcf.recode.vcf --keep NC.inds --site-pi --out NC.pi
        vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF0.01_Miss0.8.vcf.recode.vcf --keep WA.inds --site-pi --out WA.pi
        
In R:

        ### Examining pi for each population ###

        IT.pi <- read.delim("~/Documents/Academic-Research/UVM/Courses/Ecological Genomics/Assignments-Project/Assignment 2 PopStucture/IT.pi.sites.pi")
        NC.pi <- read.delim("~/Documents/Academic-Research/UVM/Courses/Ecological Genomics/Assignments-Project/Assignment 2 PopStucture/NC.pi.sites.pi")
        WA.pi <- read.delim("~/Documents/Academic-Research/UVM/Courses/Ecological Genomics/Assignments-Project/Assignment 2 PopStucture/WA.pi.sites.pi")
        
        summary(IT.pi$PI)
        sd(IT.pi$PI)
        summary(NC.pi$PI)
        sd(NC.pi$PI)
        summary(WA.pi$PI)
        sd(WA.pi$PI)
        
        IT.pi$POP = "IT"
        NC.pi$POP = "NC"
        WA.pi$POP = "WA"
        All.pi = rbind(IT.pi, NC.pi, WA.pi) #combine these into one data frame
        
        fit = aov(All.pi$PI ~ All.pi$POP) #test for sig. differences
        summary(fit)
        TukeyHSD(fit)
        
        p1=ggplot(All.pi, aes(PI, fill=POP)) + geom_histogram(binwidth = 0.035, position="dodge") + expand_limits(x = c(0, 0.5), y = c(0, 17000)) + labs(x="Site Nucleotide Diversity", y="Count") + theme_bw() + theme(legend.position="right") + geom_linerange(x=0.2141, ymax=20000, ymin=0, colour="cadetblue3", linetype=1) + geom_linerange(x=0.2027, ymax=20000, ymin=0, colour="springgreen4", linetype=1) + geom_linerange(x=0.223, ymax=20000, ymin=0, colour="red3", linetype=1) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + geom_hline(yintercept=0) #plot all in single histogram


------
<div id='id-section16'/>
### Page 16: 2018-03-26: Continuing with Bayescan

With any Bayesian analyses, it's very very important to examine the results of the MCMC run to make sure it actually worked how we think it did. To do so, we copied the relevant output files from Bayescan to our local computer as well as SNP position information:

        vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF0.01_Miss0.8.vcf.recode.vcf --kept-sites  #to get SNP site info
        
Next, we examined the output files in R:

        setwd("~/Documents/Academic-Research/UVM/Courses/Ecological Genomics/Data/Pop_Genomics/Bayescan")

        ### Import relevant output files from Bayescan ###
        Sites <- read.delim("~/Documents/Academic-Research/UVM/Courses/Ecological Genomics/Data/Pop_Genomics/Bayescan/out.kept.sites")
        
        Bayescan10_fst <- read.csv("~/Documents/Academic-Research/UVM/Courses/Ecological Genomics/Data/Pop_Genomics/Bayescan/Bayescan10_fst.txt", sep="")
        Bayescan10_LL <- read.csv("~/Documents/Academic-Research/UVM/Courses/Ecological Genomics/Data/Pop_Genomics/Bayescan/Bayescan10.sel", sep="")

        ### Check MCMC ###
        
        plot(Bayescan10_LL$logL, type="o", col="grey", ylab="Log Likelihood")
        #want this plot to look like a 'fuzzy caterpillar', including NOT having an increasing trend at the beginning which would indicate that the burn-in was not high enough
        
        par(mfrow=c(3,1), mar=c(2,2,2,1)) #check out all three populations
        plot(Bayescan10_LL$Fst1, type="o", col="grey", ylab="Fst1")
        plot(Bayescan10_LL$Fst2, type="o", col="blue", ylab="Fst2")
        plot(Bayescan10_LL$Fst3, type="o", col="red", ylab="Fst3")
        
        summary(Bayescan10_fst) #want to see whether there are any SNPs that show evidence of selection


------
<div id='id-section17'/>
### Page 17: 2018-03-28: Identifying outliers from Bayescan

Today, we continued our analysis with Bayescan, including updating the R script:

        setwd("~/Documents/Academic-Research/UVM/Courses/Ecological Genomics/Data/Pop_Genomics/Bayescan")

        ### Import relevant output files from Bayescan ###
        
        Sites <- read.delim("~/Documents/Academic-Research/UVM/Courses/Ecological Genomics/Data/Pop_Genomics/Bayescan/out.kept.sites")
        
        #these runs used a prior of 1 SNP in 1000bp assumed to be under selection
        #LL files include sampled likelihood values for each population
        #fst files include Fst values for all included loci, want to focus on alpha which indicate effect of selection on locus accounting for expected Fst due to population structure
        
        Bayescan10_fst <- read.csv("~/Documents/Academic-Research/UVM/Courses/Ecological Genomics/Data/Pop_Genomics/Bayescan/Bayescan10_fst.txt", sep="")
        Bayescan10_LL <- read.csv("~/Documents/Academic-Research/UVM/Courses/Ecological Genomics/Data/Pop_Genomics/Bayescan/Bayescan10.sel", sep="")
        
        Bayescan20_fst <- read.csv("~/Documents/Academic-Research/UVM/Courses/Ecological Genomics/Data/Pop_Genomics/Bayescan/Bayescan20_fst.txt", sep="")
        Bayescan20_LL <- read.csv("~/Documents/Academic-Research/UVM/Courses/Ecological Genomics/Data/Pop_Genomics/Bayescan/Bayescan20.sel", sep="")
        
        
        ### Check MCMC ###
        
        plot(Bayescan10_LL$logL, type="o", col="grey", ylab="Log Likelihood", xlim=c(0,200))
        #want this plot to look like a 'fuzzy catepillar', including NOT having an increasing trend at the beginning which would indicate that the burn-in was not high enough, also make sure there isn't alot of autocorrelation in ll samples
        
        par(mfrow=c(3,1), mar=c(2,2,2,1)) #check out all three populations
        plot(Bayescan10_LL$Fst1, type="o", col="grey", ylab="Fst1")
        plot(Bayescan10_LL$Fst2, type="o", col="blue", ylab="Fst2")
        plot(Bayescan10_LL$Fst3, type="o", col="red", ylab="Fst3")
        
        par(mfrow=c(3,1), mar=c(2,2,2,1)) #check out all three populations
        plot(Bayescan20_LL$Fst1, type="o", col="grey", ylab="Fst1")
        plot(Bayescan20_LL$Fst2, type="o", col="blue", ylab="Fst2")
        plot(Bayescan20_LL$Fst3, type="o", col="red", ylab="Fst3")
        
        
        ### Check summary stats of Fst values across loci ###
        
        summary(Bayescan10_fst) #want to see whether there are any SNPs that show evidence of selection
        summary(Bayescan20_fst)
        
        
        ### Combine locus info with Fst data ###
        
        Fst_sites_10 = cbind(Sites, Bayescan10_fst)
        Fst_sites_20 = cbind(Sites, Bayescan20_fst)
        
        Fst_sites_10$log10q = -log10(Fst_sites_10$qval)
        Fst_sites_20$log10q = -log10(Fst_sites_20$qval)
        
        FDR = 0.01
        
        candsnps_pr1000 = Fst_sites_10[which(Fst_sites_10$log10q>(-log10(FDR))),]
        dim(candsnps_pr1000) #check number of SNPs
        table(droplevels(candsnps_pr1000$CHROM)) #returns transcript for outliers
        
        plot(Fst_sites_10$fst, Fst_sites_10$alpha, xlab="Fst", ylab="alpha")
        points(candsnps_pr1000$fst, candsnps_pr1000$alpha, col="red", pch=16) #overlay sig. SNPs
        
        write.table(candsnps_pr1000$CHROM, "Bayescan10_pr1000_candSNPs.txt", quote=F, row.names=F, col.names=F) #export transcript list
        
Back in the terminal, we imported the "Bayescan10_pr1000_candSNPs.txt" file for further analysis. This included searching the O. taurus gene annotation file for potential functions of the genes in which the outlier SNP occur. Here, this is saying to search for the items in the "Bayescan10_pr1000_candSNPs.txt" file in the "/data/poject_data/OTAU.Models.gff3" file, and then search within those results for only annotations for "gene". Finally, then export those results to a new file. 

        grep -f ~/mydata/SNPs/Bayescan10_pr1000_candSNPs.txt /data/poject_data/OTAU.Models.gff3 | grep "gene" > ~/mydata/SNPs/Bayescan10_pr1000_candGenes.txt 

------
<div id='id-section18'/>
### Page 18: 2018-04-13: Code for class project

As demonstrated by Charlesworth et al. (1993), both directional and purifying selection can produce similar patterns of nucleotide diversity within genomes. In particular, selective sweeps and background selection (BGS) reduce neutral diversity in surrounding regions linked to mutations under selection. Especially because most mutations are expected to be deleterious, we suspect that some/many cases interpreted as positive selection may actually represent BGS. To test this as a class project, we will further examine how demographic history and different types of selection may have shaped genomic data from three sampled populations of horned beetles (Onthophagus taurus). For our part, we will identify mutations that are most likely to be deleterious and therefore subject to purifying selection. We expect that the amount of likely deleterious mutations will scale with the estimated effective population size (Ne), such that the highest number will be retained in the population with lowest Ne. 

To begin with, we first need to make sure we have all the data. This includes reads from all three O. taurus populations, as well as from 3 outgroup species which we will use to call ancestral vs. derived alleles at SNP sites. These outgroups will be 2 additional Onthophagus species and related Oryctes borbonicus (RNAseq data available from NCBI SRA). We plan to map all of the reads/data to the same reference O. taurus transcriptome so as to get SNPs at the same sites across all taxa. 

To download O. borbonicus data to the server, we first installed ascp and then called:

        ~/.aspera/connect/bin/ascp -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh -k 1 -T -l200m anonftp@ftp.ncbi.nlm.nih.gov:/sra/sra-instant/reads/ByRun/sra/SRR/SRR297/SRR2970555/SRR2970555.sra /data/project_data/beetles/share/
        
This created a .sra file, so we used SRA Toolkit to convert to .gz:

        fastq-dump  --gzip --split-files --skip-technical --read-filter pass SRR2970555.sra
        #Read 130969021 spots for SRR2970555.sra
        #Written 119147069 spots for SRR2970555.sra
        
        zcat SRR2970555_pass_1.fastq.gz | head -n20   #def fastq but looked kind of weird...
        zcat SRR2970555_pass_2.fastq.gz | head -n20   #def fastq but seems to be NNN only

------
<div id='id-section19'/>
### Page 19: 2018-04-16: Continuing with analysis for class project

Used FastQC to check overall quality of the split fastq files:        
        
        fastqc /data/project_data/beetles/share/SRR2970555_pass_1.fastq.gz -o .
        fastqc /data/project_data/beetles/share/SRR2970555_pass_2.fastq.gz -o .
        
They actually looked pretty good, so I continued via Trimmomatic: 

        #!/bin/bash
      java -classpath /data/popgen/Trimmomatic-0.33/trimmomatic-0.33.jar org.usadellab.trimmomatic.TrimmomaticPE \
                -threads 1 \
                -phred33 \
                 /data/project_data/beetles/share/SRR2970555_pass_1.fastq.gz \
                 /data/project_data/beetles/share/SRR2970555_pass_2.fastq.gz \
                 ~/mydata/2018_clean_reads/"SRR2970555_R1_clean_paired.fa" \
                 ~/mydata/2018_clean_reads/"SRR2970555_R1_clean_unpaired.fa" \
                 ~/mydata/2018_clean_reads/"SRR2970555_R2_clean_paired.fa" \
                 ~/mydata/2018_clean_reads/"SRR2970555_R2_clean_unpaired.fa" \
                 ILLUMINACLIP:/data/popgen/Trimmomatic-0.33/adapters/TruSeq3-PE.fa:2:30:10 \
                 LEADING:28 \
             TRAILING:28 \
             SLIDINGWINDOW:6:28 \
             HEADCROP:12 \
             MINLEN:35 \
             
        #Output: Using PrefixPair: 'TACACTCTTTCCCTACACGACGCTCTTCCGATCT' and 'GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT'
        ILLUMINACLIP: Using 1 prefix pairs, 0 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences
        Input Read Pairs: 119147069 Both Surviving: 97273061 (81.64%) Forward Only Surviving: 9746129 (8.18%) Reverse Only Surviving: 3756697 (3.15%) Dropped: 8371182 (7.03%)
        TrimmomaticPE: Completed successfully    
        
Checked quality again:

        fastqc SRR2970555_R1_clean_paired.fa
        fastqc SRR2970555_R2_clean_paired.fa
        
And mapped to O. tau reference using both aln and mem functions, for comparison:

        bwa aln /data/project_data/beetles/reference/OTAU.fna  ~/mydata/2018_clean_reads/SRR2970555_R1_clean_paired.fa  ~/mydata/2018_clean_reads/SRR2970555_R2_clean_paired.fa > Obor_SRR_bwaaln.sam
        
However, when I went to check this .sam file with flagstat, I got an error message that it wasn't in the correct format. But the mem function worked okay:
        
        bwa mem /data/project_data/beetles/reference/OTAU.fna  ~/mydata/2018_clean_reads/SRR2970555_R1_clean_paired.fa  ~/mydata/2018_clean_reads/SRR2970555_R2_clean_paired.fa > Obor_SRR_bwamem.sam
             
        samtools flagstat Obor_SRR_bwamem.sam
        #Ouput: 194548022 + 0 in total (QC-passed reads + QC-failed reads)
        0 + 0 secondary
        1900 + 0 supplementary
        0 + 0 duplicates
        17708031 + 0 mapped (9.10% : N/A)
        194546122 + 0 paired in sequencing
        97273061 + 0 read1
        97273061 + 0 read2
        15771066 + 0 properly paired (8.11% : N/A)
        15860486 + 0 with itself and mate mapped
        1845645 + 0 singletons (0.95% : N/A)
        63106 + 0 with mate mapped to a different chr
        46653 + 0 with mate mapped to a different chr (mapQ>=5)    
        
Just to see if it would work, I also mapped the assembled reads from the other species against the same reference using the same approach:

        bwa mem /data/project_data/beetles/reference/OTAU.fna  /data/project_data/beetles/assemblies/Osag_TrinityClean.fa.gz > Osag_bwamem.sam
        
        bwa mem /data/project_data/beetles/reference/OTAU.fna  /data/project_data/beetles/assemblies/Onig_all_09272011.fa.gz > Onig_bwamem.sam
        
        samtools flagstat Onig_bwamem.sam
        #Output: 115761 + 0 in total (QC-passed reads + QC-failed reads)
        0 + 0 secondary
        15462 + 0 supplementary
        0 + 0 duplicates
        85042 + 0 mapped (73.46% : N/A)
        0 + 0 paired in sequencing
        0 + 0 read1
        0 + 0 read2
        0 + 0 properly paired (N/A : N/A)
        0 + 0 with itself and mate mapped
        0 + 0 singletons (N/A : N/A)
        0 + 0 with mate mapped to a different chr
        0 + 0 with mate mapped to a different chr (mapQ>=5)
        
        samtools flagstat Osag_bwamem.sam
        #Output: 96476 + 0 in total (QC-passed reads + QC-failed reads)
        0 + 0 secondary
        19712 + 0 supplementary
        0 + 0 duplicates
        57329 + 0 mapped (59.42% : N/A)
        0 + 0 paired in sequencing
        0 + 0 read1
        0 + 0 read2
        0 + 0 properly paired (N/A : N/A)
        0 + 0 with itself and mate mapped
        0 + 0 singletons (N/A : N/A)
        0 + 0 with mate mapped to a different chr
        0 + 0 with mate mapped to a different chr (mapQ>=5)
             
------
<div id='id-section20'/>
### Page 20: 2018-04-17: Code for class project, continued

After getting sam files for each of the outgroups, I created a script to get the appropriately filtered and sorted bam files:

        vim BAM_processing_project.sh
        
        #!/bin/bash

        samtools view -bS -@ 4 Osag_bwamem.sam > Osag_bwamem.bam
        samtools flagstat Osag_bwamem.bam
        samtools depth Osag_bwamem.bam
        samtools fixmate Osag_bwamem.bam Osag_bwamem.fm.bam
        samtools sort -@ 4 Osag_bwamem.fm.bam -o Osag_bwamem.fm.sort.bam
        samtools rmdup Osag_bwamem.fm.sort.bam Osag_bwamem.fm.sort.rmdup.bam
        samtools index Osag_bwamem.fm.sort.rmdup.bam
        
        samtools view -bS -@ 4 Onig_bwamem.sam > Onig_bwamem.bam
        samtools flagstat Onig_bwamem.bam
        samtools depth Onig_bwamem.bam
        samtools fixmate Onig_bwamem.bam Onig_bwamem.fm.bam
        samtools sort -@ 4 Onig_bwamem.fm.bam -o Onig_bwamem.fm.sort.bam
        samtools rmdup Onig_bwamem.fm.sort.bam  Onig_bwamem.fm.sort.rmdup.bam
        samtools index Onig_bwamem.fm.sort.rmdup.bam
        
        samtools view -bS -@ 4 Obor_SRR_bwamem.sam > Obor_SRR_bwamem.bam
        samtools flagstat Obor_SRR_bwamem.bam
        samtools depth Obor_SRR_bwamem.bam
        samtools fixmate Obor_SRR_bwamem.bam Obor_SRR_bwamem.fm.bam
        samtools sort -@ 4 Obor_SRR_bwamem.fm.bam -o Obor_SRR_bwamem.fm.sort.bam
        samtools rmdup Obor_SRR_bwamem.fm.sort.bam Obor_SRR_bwamem.fm.sort.rmdup.bam
        samtools index Obor_SRR_bwamem.fm.sort.rmdup.bam

------
<div id='id-section21'/>
### Page 21: 2018-04-18: Code for class project, continued

Today, I moved all of the sorted and indexed .bam files to our shared folder, as well as the merged .bam file for all individuals from the 3 O. taurus populations (Melissa provided this for us). Next, I renamed and indexed the merged O. taurus file, as well as created a bam list file to use in mpileup:

        mv merged_fixmate.sorted.rmdup.bam Otau_fm.sort.rmdup.bam
        samtools index Otau_fm.sort.rmdup.bam
        
        vim BAM_List.txt #each file must be on its own line
        #/data/project_data/beetles/share/EthanJamie/Obor_SRR_bwamem.fm.sort.rmdup.bam
        /data/project_data/beetles/share/EthanJamie/Onig_bwamem.fm.sort.rmdup.bam
        /data/project_data/beetles/share/EthanJamie/Osag_bwamem.fm.sort.rmdup.bam
        /data/project_data/beetles/share/EthanJamie/Otau_fm.sort.rmdup.bam
        
Then I created a bash script to run mpileup. This script calls to the O. tau reference transcriptome, specifies the input bam files, and says to output the consensus call for all sites as a vcf:

        vim CallSNPs_project.sh
        # #!/bin/bash

        bcftools mpileup -Ou -f /data/project_data/beetles/reference/OTAU.fna --bam-list BAM_List.txt  –-threads 8 –-skip-indel –-annotate AD,DP | bcftools call -c  > All_Taxa_Consensus.vcf
        

------
<div id='id-section22'/>
### Page 22: 2018-04-19-2018: Class project (Calling ancestral vs. derived alleles)

The All_Taxa_Consensus.vcf files contains the consensus sequence across taxa at all sites. To retain only the sites where there are SNPs in O. taurus, I moved our working Otau SNP file to the same folder and outputed the SNP positions to a file called Otau_SNPs.kept.sites. Then I told vcftools to retain only those sites from the consensus vcf file:

        vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF0.01_Miss0.8.vcf.recode.vcf --kept-sites --out Otau_SNPs

        vcftools --vcf All_Taxa_Consensus.vcf --positions Otau_SNPs.kept.sites --recode --out All_Taxa_OtauSNP_Sites


------
<div id='id-section23'/>
### Page 23: 2018-04-23: Ancestral vs. derived alleles

Today, we realized that we should call the overall consensus using equal representation of each species, not a single individual of the 3 outgroups vs. 72 of O. taurus. So re-ran this calling a consensus of O. taurus first:

        bcftools mpileup  --threads 15 --skip-indel --annotate AD,DP -Ou -f /data/project_data/beetles/reference/OTAU.fna /data/project_data/beetles/share/EthanJamie/Otau_fm.sort.rmdup.bam | bcftools call -c  > All_Otau_Consensus.vcf
        vcfutils.pl vcf2fq All_Otau_Consensus.vcf > Otau_Consensus.fq
        
Alternatively, could have just used this:       
        
        bcftools mpileup  --threads 15 --skip-indel --annotate AD,DP -Ou -f /data/project_data/beetles/reference/OTAU.fna /data/project_data/beetles/share/EthanJamie/Otau_fm.sort.rmdup.bam | bcftools call -c | vcfutils.pl vcf2fq > Otau_consensus.fq
        
Then, need to convert the .fastq back to .bam to use in mpileup function:

      bwa mem /data/project_data/beetles/reference/OTAU.fna  /data/project_data/beetles/share/EthanJamie/Otau_Consensus.fq > Otau_Consensus.sam
      samtools view -bS -@ 4 Otau_Consensus.sam > Otau_Consensus.bam
      samtools sort -@ 4 Otau_Consensus.bam -o Otau_Consensus.sort.bam
      samtools index Otau_Consensus.sort.bam
      
Then, update bam_list.txt and re-run mpileup via CallSNPs_project.sh:

        /data/project_data/beetles/share/EthanJamie/Obor_SRR_bwamem.fm.sort.rmdup.bam
        /data/project_data/beetles/share/EthanJamie/Onig_bwamem.fm.sort.rmdup.bam
        /data/project_data/beetles/share/EthanJamie/Osag_bwamem.fm.sort.rmdup.bam
        /data/project_data/beetles/share/EthanJamie/Otau_Consensus.sort.bam
        
        bash CallSNPs_project.sh
        

Used the following to look at vcf files:

        head -n50000 All_Taxa_Consensus2.vcf | tail -n10 > Test.out
        vim Test.out
        
And then pulled out only the sites where we previously identified SNPs in Otau:        
        
        vcftools --vcf All_Taxa_Consensus2.vcf --positions Otau_SNPs.kept.sites --recode --out All_Taxa_OtauSNP_Sites
        
We then pulled out base sequence at all sites in consensus (=ancestral allele), as well as in O. taurus, and merged them into a single file: "AncestralDerivedAlleles.txt". 
        
        grep -v "#" All_Taxa_OtauSNP_Sites.recode.vcf | cut -f 1,2,4,5 > Consensus_Bases.txt
        grep -v "#" OTAU_2018_reads2snps_DP10GP95_biallelic_MAF0.01_Miss0.8.vcf.recode.vcf | cut -f 1,2,4,5 > Otaurus_Bases.txt
        
To create population specific vcf files:

        vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF0.01_Miss0.8.vcf.recode.vcf --keep IT.inds --recode --out IT_SNPs
        vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF0.01_Miss0.8.vcf.recode.vcf --keep NC.inds --recode --out NC_SNPs
        vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF0.01_Miss0.8.vcf.recode.vcf --keep WA.inds --recode --out WA_SNPs
        
------
<div id='id-section24'/>
### Page 24: 2018-04-25: Class project (Identifying genes where snps are mostly likely to be bad)

I tried working with vcf2eqtl in R, but the program didn't like how our working vcf file was formatted, in particular, it required the file to have additional genotype tags. To work around this, we pulled out the SNP site positions from the vcf file created using samtools (as reads2snps doesn't output the needed tags):

        vcftools --vcf /data/project_data/beetles/snps/OTAU_2018_samtools.vcf --min-alleles 2 --max-alleles 2 --positions Otau_SNPs.kept.sites --recode --out OtauSNPs.eQTL
        
This should have worked, but it created a vcf file with only 115511 sites compared to the 133291 that should have been retained. Ok, it's just that samtools retained fewer sites during the initial mapping. Will just use this for now. Here is my R code for the eQTL analysis:

        library("vcf2eqtl", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
        
        counts <- read.delim('allcountsdataRN.txt', header=T, stringsAsFactors = T, row.names=1)
        counts = as.matrix(counts)

        vcf <- 'OtauSNPs.eQTL.recode.vcf'
        
        metadata<-read.delim('cols_data.txt')
        
        pops=metadata$population
        
        results = vcf2eqtl(vcf, counts, pops, mc.cores = 2)
        view(results)
        results2 = results[["res"]]
        ContigExpr = results[["snpContigExpr"]]
        geno = results[["genos"]]
        write.csv(results2, file="eQTL_Output.csv")
        write.csv(ContigExpr, file="eQTL_contigExpr.csv")
        write.csv(geno, file="eQTL_genotypes.csv")
        

To identify SNPs with predicted deleterious effects:

        grep -e "MODERATE" -e "HIGH" /data/project_data/beetles/share/snpeff/snpeffcontigs.biallelic.vcf.recode.vcf  > Bad_SNPs.vcf
        
        vcftools --vcf Bad_SNPs.vcf --kept-sites --out Bad_SNPs_sites
        
        
To identify SNPs in highly expressed transcripts, I created tab delimited txt files for each population that contained the top 200 most highly expressed transcripts, using the count data in allcountsdataRN.txt. Then I uploaded them to the server, and made sure they were in the correct format:

        vim IT_Top200.txt
        #:%s/<Ctrl-V><Ctrl-M>/\r/g where <Ctrl-V><Ctrl-M> means type Ctrl+V then Ctrl+M to convert line breaks


To compare genes and SNPs from the "bad" list to the top 200 highest expressed lists as well as eQTL list:

        grep -f WA_Top200.txt Bad_SNPs_sites.kept.sites > WA_Bad_Genes_Top200.txt
        grep -f IT_Top200.txt Bad_SNPs_sites.kept.sites > IT_Bad_Genes_Top200.txt
        grep -f NC_Top200.txt Bad_SNPs_sites.kept.sites > NC_Bad_Genes_Top200.txt
        
        vcftools --vcf WA_SNPs.recode.vcf --positions WA_Bad_Genes_Top200.txt  --recode --out WA_Bad_SNPs_Top200
        vcftools --vcf IT_SNPs.recode.vcf --positions IT_Bad_Genes_Top200.txt  --recode --out IT_Bad_SNPs_Top200
        vcftools --vcf NC_SNPs.recode.vcf --positions NC_Bad_Genes_Top200.txt  --recode --out NC_Bad_SNPs_Top200
        
        vcftools --vcf WA_SNPs.recode.vcf --positions eQTL_sites.txt  --recode --out WA_Bad_SNPs_eQTL
        vcftools --vcf IT_SNPs.recode.vcf --positions eQTL_sites.txt  --recode --out IT_Bad_SNPs_eQTL
        vcftools --vcf NC_SNPs.recode.vcf --positions eQTL_sites.txt  --recode --out NC_Bad_SNPs_eQTL
        

To examine allele frequencies of ancestral vs. derived at these sites among populations:        
        
        grep -v "#" WA_Bad_SNPs_eQTL.recode.vcf | cut -f 1,2,4,5 > WA_Bad_eQTL_Bases.txt
        grep -v "#" WA_Bad_SNPs_Top200.recode.vcf | cut -f 1,2,4,5 > WA_Bad_Top200_Bases.txt
        grep -v "#" IT_Bad_SNPs_eQTL.recode.vcf | cut -f 1,2,4,5 > IT_Bad_eQTL_Bases.txt
        grep -v "#" IT_Bad_SNPs_Top200.recode.vcf | cut -f 1,2,4,5 > IT_Bad_Top200_Bases.txt
        grep -v "#" NC_Bad_SNPs_eQTL.recode.vcf | cut -f 1,2,4,5 > NC_Bad_eQTL_Bases.txt
        grep -v "#" NC_Bad_SNPs_Top200.recode.vcf | cut -f 1,2,4,5 > NC_Bad_Top200_Bases.txt
        
        vcftools --vcf WA_Bad_SNPs_Top200.recode.vcf --freq --out WA_Bad_Top200
        vcftools --vcf WA_Bad_SNPs_eQTL.recode.vcf --freq --out WA_Bad_eQTL
        vcftools --vcf IT_Bad_SNPs_Top200.recode.vcf --freq --out IT_Bad_Top200
        vcftools --vcf IT_Bad_SNPs_eQTL.recode.vcf --freq --out IT_Bad_eQTL
        vcftools --vcf NC_Bad_SNPs_Top200.recode.vcf --freq --out NC_Bad_Top200
        vcftools --vcf NC_Bad_SNPs_eQTL.recode.vcf --freq --out NC_Bad_eQTL
        
        
        







------
<div id='id-section25'/>
### Page 25:
------
<div id='id-section26'/>
### Page 26:
------
<div id='id-section27'/>
### Page 27:
------
<div id='id-section28'/>
### Page 28:
------
<div id='id-section29'/>
### Page 29:
------
<div id='id-section30'/>
### Page 30:
------
<div id='id-section31'/>
### Page 31:
------
<div id='id-section32'/>
### Page 32:
------
<div id='id-section33'/>
### Page 33:
------
<div id='id-section34'/>
### Page 34:
------
<div id='id-section35'/>
### Page 35:
------
<div id='id-section36'/>
### Page 36:
------
<div id='id-section37'/>
### Page 37:
------
<div id='id-section38'/>
### Page 38:
------
<div id='id-section39'/>
### Page 39:
------
<div id='id-section40'/>
### Page 40:
------
<div id='id-section41'/>
### Page 41:
------
<div id='id-section42'/>
### Page 42:
------
<div id='id-section43'/>
### Page 43:
------
<div id='id-section44'/>
### Page 44:
------
<div id='id-section45'/>
### Page 45:
------
<div id='id-section46'/>
### Page 46:
------
<div id='id-section47'/>
### Page 47:
------
<div id='id-section48'/>
### Page 48:
------
<div id='id-section49'/>
### Page 49:
------
<div id='id-section50'/>
### Page 50:
------
<div id='id-section51'/>
### Page 51:
------
<div id='id-section52'/>
### Page 52:
------
<div id='id-section53'/>
### Page 53:
------
<div id='id-section54'/>
### Page 54:
------
<div id='id-section55'/>
### Page 55:
------
<div id='id-section56'/>
### Page 56:
------
<div id='id-section57'/>
### Page 57:
------
<div id='id-section58'/>
### Page 58:
------
<div id='id-section59'/>
### Page 59:
------
<div id='id-section60'/>
### Page 60:

------
